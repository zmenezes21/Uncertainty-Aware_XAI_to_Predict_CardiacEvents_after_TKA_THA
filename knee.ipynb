{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZIP1N-q0g0pq"
      },
      "outputs": [],
      "source": [
        "pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqciV8XAGCvb"
      },
      "outputs": [],
      "source": [
        "pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cb8174c7-9d12-4120-9eb6-93c76188fae9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score, roc_auc_score, recall_score,f1_score, precision_recall_curve, roc_curve, auc\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import shap\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "import numpy as np\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "from scipy import stats\n",
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkTGv4fvQ1ry"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-rBDiBP8nZt"
      },
      "source": [
        "# Cardiac Arrest Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsiNRTPGRdhK"
      },
      "outputs": [],
      "source": [
        "dfca = pd.read_csv('/content/drive/My Drive/capstone/undersampled_CDARREST_Knee_21_22_23(2).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f4867443-409d-4ee8-9cbd-e082fbf075e4"
      },
      "outputs": [],
      "source": [
        "dfca.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yi3UsF4483XX"
      },
      "source": [
        "# CDMI Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VaU7Qtj9D8K"
      },
      "outputs": [],
      "source": [
        "dfcdmi = pd.read_csv('/content/drive/My Drive/capstone/undersampled_CDMI_Knee_21_22_23(2).csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2LfXd4JnRpj"
      },
      "outputs": [],
      "source": [
        "dfcdmi.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46536a9e-fcec-4938-8c3b-f357a49fa90c"
      },
      "source": [
        "# Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cad5c14a-97e5-49c3-88ec-f515d853d931"
      },
      "source": [
        "## Cardiac Arrest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds and evaluates a Random Forest model to predict cardiac arrest (CDARREST). Optuna is used to tune hyperparameters for the model by maximizing the area under the ROC curve (AUC) through 5-fold cross-validation. After finding the best parameters, it retrains the model, evaluates its performance (AUC, accuracy, F1, recall, precision), and plots the ROC curve to visualize how well the model distinguishes between positive and negative cases."
      ],
      "metadata": {
        "id": "DzMBPo5DQfBI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J4P17MOpfoI"
      },
      "outputs": [],
      "source": [
        "X_RF_CA = dfca.drop(columns=['CDARREST'])\n",
        "y_RF_CA = dfca['CDARREST']\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "\n",
        "    rf_classifier_CA = RandomForestClassifier(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        min_samples_split=min_samples_split,\n",
        "        min_samples_leaf=min_samples_leaf,\n",
        "        max_features=max_features,\n",
        "        class_weight='balanced',\n",
        "        random_state=42\n",
        ")\n",
        "\n",
        "    kf_RF_CA = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    y_pred_prob_RF_CA = cross_val_predict(rf_classifier_CA, X_RF_CA, y_RF_CA, cv=kf_RF_CA, method=\"predict_proba\")[:, 1]\n",
        "\n",
        "    fpr_RF_CA, tpr_RF_CA, _ = roc_curve(y_RF_CA, y_pred_prob_RF_CA)\n",
        "    roc_auc_RF_CA = auc(fpr_RF_CA, tpr_RF_CA)\n",
        "\n",
        "    return roc_auc_RF_CA\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print('Best hyperparameters: ', study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "rf_classifier_CA = RandomForestClassifier(**best_params, random_state=42)\n",
        "\n",
        "kf_RF_CA = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "y_pred_prob_RF_CA = cross_val_predict(rf_classifier_CA, X_RF_CA, y_RF_CA, cv=kf_RF_CA, method=\"predict_proba\")[:, 1]\n",
        "y_pred_RF_CA = cross_val_predict(rf_classifier_CA, X_RF_CA, y_RF_CA, cv=kf_RF_CA, method=\"predict\")\n",
        "\n",
        "fpr_RF_CA, tpr_RF_CA, _ = roc_curve(y_RF_CA, y_pred_prob_RF_CA)\n",
        "roc_auc_RF_CA = auc(fpr_RF_CA, tpr_RF_CA)\n",
        "\n",
        "accuracy_RF_CA = accuracy_score(y_RF_CA, y_pred_RF_CA)\n",
        "f1_RF_CA = f1_score(y_RF_CA, y_pred_RF_CA)\n",
        "recall_RF_CA = recall_score(y_RF_CA, y_pred_RF_CA)\n",
        "precision_RF_CA = precision_score(y_RF_CA, y_pred_RF_CA)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_RF_CA, tpr_RF_CA, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_RF_CA:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest (Cardiac Arrest)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'Random Forest Metrics for Cardiac Arrest: ')\n",
        "print(f'AUC Score: {roc_auc_RF_CA:.4f}')\n",
        "print(f'Accuracy: {accuracy_RF_CA:.4f}')\n",
        "print(f'F1 Score: {f1_RF_CA:.4f}')\n",
        "print(f'Recall: {recall_RF_CA:.4f}')\n",
        "print(f'Precision: {precision_RF_CA:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Rekoa1bBN80"
      },
      "outputs": [],
      "source": [
        "rf_classifier_CA.fit(X_RF_CA, y_RF_CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code generates a SHAP summary plot to interpret the Random Forest model predicting cardiac arrest. It uses SHAP (SHapley Additive exPlanations) to calculate each feature's contribution to the model's predictions. The summary plot visually ranks features by importance and shows how they impact the output, helping to understand which variables most influence the risk of cardiac arrest."
      ],
      "metadata": {
        "id": "xNaKKcTaRZ8H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygkfANUzrt4D"
      },
      "outputs": [],
      "source": [
        "explainer_RF_CA = shap.TreeExplainer(rf_classifier_CA)\n",
        "shap_values_RF_CA = explainer_RF_CA.shap_values(X_RF_CA)\n",
        "\n",
        "\n",
        "feature_names = X_RF_CA.columns\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "shap.summary_plot(shap_values_RF_CA, X_RF_CA, feature_names=feature_names, plot_size=(2, 0.5), show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - Random Forest (CA)\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.2, bottom=0.2, left=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code creates a LIME (Local Interpretable Model-agnostic Explanations) explanation for a single, randomly selected patient in the cardiac arrest dataset. It explains how the Random Forest model made its prediction for that specific case by showing the contribution of individual features to the predicted probability. The resulting plot visually breaks down which features pushed the prediction toward or away from a cardiac arrest classification."
      ],
      "metadata": {
        "id": "mAsCXOLoRsIW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waOEc-DAxQHp"
      },
      "outputs": [],
      "source": [
        "lime_explainer_RF_CA = LimeTabularExplainer(\n",
        "    training_data=X_RF_CA.values,\n",
        "    feature_names=X_RF_CA.columns.tolist(),\n",
        "    class_names=['No Arrest', 'Arrest'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "idx_RF_CA = np.random.randint(0, X_RF_CA.shape[0])\n",
        "lime_exp_RF_CA = lime_explainer_RF_CA.explain_instance(\n",
        "    X_RF_CA.iloc[idx_RF_CA],\n",
        "    rf_classifier_CA.predict_proba\n",
        ")\n",
        "\n",
        "lime_exp_RF_CA.show_in_notebook()\n",
        "\n",
        "fig = lime_exp_RF_CA.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "717ab67a-48ca-4b60-a665-cdb786354183"
      },
      "source": [
        "## CDMI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarly, this code builds and evaluates a Random Forest model to predict myocardial infarction (CDMI). Optuna is used to tune hyperparameters for the model by maximizing the area under the ROC curve (AUC) through 5-fold cross-validation. After finding the best parameters, it retrains the model, evaluates its performance (AUC, accuracy, F1, recall, precision), and plots the ROC curve to visualize how well the model distinguishes between positive and negative cases."
      ],
      "metadata": {
        "id": "JHxsG1EGQ1dX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRQVgyVapmp3"
      },
      "outputs": [],
      "source": [
        "X_RF_CDMI = dfcdmi.drop(columns=['CDMI'])\n",
        "y_RF_CDMI = dfcdmi['CDMI']\n",
        "\n",
        "def objective(trial):\n",
        "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
        "    max_depth = trial.suggest_int('max_depth', 3, 20)\n",
        "    min_samples_split = trial.suggest_int('min_samples_split', 2, 20)\n",
        "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 20)\n",
        "    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n",
        "\n",
        "    rf_classifier_CDMI = RandomForestClassifier(\n",
        "      n_estimators=n_estimators,\n",
        "      max_depth=max_depth,\n",
        "      min_samples_split=min_samples_split,\n",
        "      min_samples_leaf=min_samples_leaf,\n",
        "      max_features=max_features,\n",
        "      class_weight='balanced',\n",
        "      random_state=42\n",
        "  )\n",
        "\n",
        "    kf_RF_CDMI = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    y_pred_prob_RF_CDMI = cross_val_predict(rf_classifier_CDMI, X_RF_CDMI, y_RF_CDMI, cv=kf_RF_CDMI, method=\"predict_proba\")[:, 1]\n",
        "\n",
        "    fpr_RF_CDMI, tpr_RF_CDMI, _ = roc_curve(y_RF_CDMI, y_pred_prob_RF_CDMI)\n",
        "    roc_auc_RF_CDMI = auc(fpr_RF_CDMI, tpr_RF_CDMI)\n",
        "\n",
        "    return roc_auc_RF_CDMI\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "\n",
        "print('Best hyperparameters: ', study.best_params)\n",
        "\n",
        "best_params = study.best_params\n",
        "rf_classifier_CDMI = RandomForestClassifier(**best_params, random_state=42)\n",
        "\n",
        "kf_RF_CDMI = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "y_pred_prob_RF_CDMI = cross_val_predict(rf_classifier_CDMI, X_RF_CDMI, y_RF_CDMI, cv=kf_RF_CDMI, method=\"predict_proba\")[:, 1]\n",
        "y_pred_RF_CDMI = cross_val_predict(rf_classifier_CDMI, X_RF_CDMI, y_RF_CDMI, cv=kf_RF_CDMI, method=\"predict\")\n",
        "\n",
        "fpr_RF_CDMI, tpr_RF_CDMI, _ = roc_curve(y_RF_CDMI, y_pred_prob_RF_CDMI)\n",
        "roc_auc_RF_CDMI = auc(fpr_RF_CDMI, tpr_RF_CDMI)\n",
        "\n",
        "accuracy_RF_CDMI = accuracy_score(y_RF_CDMI, y_pred_RF_CDMI)\n",
        "f1_RF_CDMI = f1_score(y_RF_CDMI, y_pred_RF_CDMI)\n",
        "recall_RF_CDMI = recall_score(y_RF_CDMI, y_pred_RF_CDMI)\n",
        "precision_RF_CDMI = precision_score(y_RF_CDMI, y_pred_RF_CDMI)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_RF_CDMI, tpr_RF_CDMI, color='blue', lw=2, label=f'Random Forest (AUC = {roc_auc_RF_CDMI:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Random Forest (CDMI)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'Random Forest Metrics for CDMI: ')\n",
        "print(f'AUC Score: {roc_auc_RF_CDMI:.4f}')\n",
        "print(f'Accuracy: {accuracy_RF_CDMI:.4f}')\n",
        "print(f'F1 Score: {f1_RF_CDMI:.4f}')\n",
        "print(f'Recall: {recall_RF_CDMI:.4f}')\n",
        "print(f'Precision: {precision_RF_CDMI:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDPIoGJQBVJs"
      },
      "outputs": [],
      "source": [
        "rf_classifier_CDMI.fit(X_RF_CDMI, y_RF_CDMI)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code generates a SHAP summary plot to interpret the Random Forest model predicting myocardial infarction. It uses SHAP (SHapley Additive exPlanations) to calculate each feature's contribution to the model's predictions. The summary plot visually ranks features by importance and shows how they impact the output, helping to understand which variables most influence the risk of myocardial infarction."
      ],
      "metadata": {
        "id": "5Wdwkn4mSIZ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LP0_Y3sWHtfg"
      },
      "outputs": [],
      "source": [
        "explainer_RF_CDMI = shap.TreeExplainer(rf_classifier_CDMI)\n",
        "shap_values_RF_CDMI = explainer_RF_CDMI.shap_values(X_RF_CDMI)\n",
        "feature_names = X_RF_CDMI.columns\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "shap.summary_plot(shap_values_RF_CDMI, X_RF_CDMI, feature_names=feature_names, plot_size=(2, 0.5), show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - Random Forest (CDMI)\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.2, bottom=0.2, left=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code creates a LIME (Local Interpretable Model-agnostic Explanations) explanation for a single, randomly selected patient in the myocardial infarction dataset. It explains how the Random Forest model made its prediction for that specific case by showing the contribution of individual features to the predicted probability. The resulting plot visually breaks down which features pushed the prediction toward or away from a myocardial infarction classification."
      ],
      "metadata": {
        "id": "KUjed3LJSZip"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xbVWSm77xV0c"
      },
      "outputs": [],
      "source": [
        "lime_explainer_RF_CDMI = LimeTabularExplainer(\n",
        "    training_data=X_RF_CDMI.values,\n",
        "    feature_names=X_RF_CDMI.columns.tolist(),\n",
        "    class_names=['No CDMI', 'CDMI'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "idx_RF_CDMI = np.random.randint(0, X_RF_CDMI.shape[0])\n",
        "lime_exp_RF_CDMI = lime_explainer_RF_CDMI.explain_instance(\n",
        "    X_RF_CDMI.iloc[idx_RF_CDMI],\n",
        "    rf_classifier_CDMI.predict_proba\n",
        ")\n",
        "\n",
        "lime_exp_RF_CDMI.show_in_notebook()\n",
        "\n",
        "fig = lime_exp_RF_CDMI.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95901bcb-e485-47fa-ac29-23b94598a9ba"
      },
      "source": [
        "## Combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code builds a combined model by merging two Random Forest classifiers of cardiac arrest and myocardial infarction. It first concatenates the feature and target datasets, ensuring consistent feature names, and then fits a soft-voting ensemble model (VotingClassifier) using both classifiers. The combined model’s performance is evaluated using standard classification metrics (accuracy, F1, recall, precision, and AUC), and the ROC curve is plotted to show its ability to distinguish between classes."
      ],
      "metadata": {
        "id": "8ajCaIsOS2rW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Fr5iWi-LcMD"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score, StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSv9UJY2MAPS"
      },
      "outputs": [],
      "source": [
        "X_RF_CA_resampled = dfca.drop('CDARREST', axis=1)\n",
        "y_RF_CA_resampled = dfca['CDARREST']\n",
        "\n",
        "X_RF_CDMI_resampled = dfcdmi.drop('CDMI', axis=1)\n",
        "y_RF_CDMI_resampled = dfcdmi['CDMI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8H6Ll-kT1Kni"
      },
      "outputs": [],
      "source": [
        "X_RF_Combined = np.concatenate((X_RF_CA_resampled, X_RF_CDMI_resampled), axis=0)\n",
        "y_RF_Combined = np.concatenate((y_RF_CA_resampled, y_RF_CDMI_resampled), axis=0)\n",
        "\n",
        "original_feature_names = X_RF_CA_resampled.columns.tolist()\n",
        "\n",
        "X_RF_Combined_df = pd.DataFrame(X_RF_Combined, columns=original_feature_names)\n",
        "\n",
        "voting_classifier_RF_Combined = VotingClassifier(estimators=[\n",
        "    ('rf_CA', rf_classifier_CA),\n",
        "    ('rf_CDMI', rf_classifier_CDMI)\n",
        "], voting='soft')\n",
        "\n",
        "voting_classifier_RF_Combined.fit(X_RF_Combined, y_RF_Combined)\n",
        "\n",
        "y_pred_combined_labels = voting_classifier_RF_Combined.predict(X_RF_Combined)\n",
        "\n",
        "accuracy_combined = accuracy_score(y_RF_Combined, y_pred_combined_labels)\n",
        "f1_combined = f1_score(y_RF_Combined, y_pred_combined_labels)\n",
        "recall_combined = recall_score(y_RF_Combined, y_pred_combined_labels)\n",
        "precision_combined = precision_score(y_RF_Combined, y_pred_combined_labels)\n",
        "\n",
        "y_pred_combined_prob = voting_classifier_RF_Combined.predict_proba(X_RF_Combined)[:, 1]\n",
        "fpr_combined, tpr_combined, _ = roc_curve(y_RF_Combined, y_pred_combined_prob)\n",
        "roc_auc_combined = auc(fpr_combined, tpr_combined)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_combined, tpr_combined, color='red', lw=2, label=f'Combined Model (AUC = {roc_auc_combined:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Combined Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'Combined Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined:.4f}')\n",
        "print(f'Recall: {recall_combined:.4f}')\n",
        "print(f'AUC Score: {roc_auc_combined:.4f}')\n",
        "print(f'Precision: {precision_combined:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code computes SHAP values for the combined dataset with both models, averages them to get a unified interpretation, and then plots a SHAP summary bar chart. This helps identify which features are most influential across both models."
      ],
      "metadata": {
        "id": "WYvNFL1LUDvx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z3yf-45mB-ym"
      },
      "outputs": [],
      "source": [
        "X_RF_Combined_df = pd.DataFrame(X_RF_Combined, columns=X_RF_CA_resampled.columns)\n",
        "\n",
        "explainer_RF_CA = shap.TreeExplainer(rf_classifier_CA)\n",
        "explainer_RF_CDMI = shap.TreeExplainer(rf_classifier_CDMI)\n",
        "\n",
        "shap_values_RF_CA = explainer_RF_CA.shap_values(X_RF_Combined)\n",
        "shap_values_RF_CDMI = explainer_RF_CDMI.shap_values(X_RF_Combined)\n",
        "\n",
        "shap_values_Combined = (shap_values_RF_CA + shap_values_RF_CDMI) / 2\n",
        "\n",
        "assert shap_values_Combined.shape[1] == X_RF_Combined_df.shape[1], \\\n",
        "    f\"Shape mismatch: SHAP values ({shap_values_Combined.shape[1]} features) and data ({X_RF_Combined_df.shape[1]} features)\"\n",
        "\n",
        "shap.summary_plot(shap_values_Combined, X_RF_Combined_df, plot_type=\"bar\", title=\"SHAP Summary Plot - Combined Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to explain how the combined model made its predictions. It sets up a LimeTabularExplainer with the combined training data and then generates a local explanation for the selected instance by showing how individual features influenced the predicted outcome. The explanation is both displayed as a table and visualized in a bar chart."
      ],
      "metadata": {
        "id": "3qrrNyPmUahj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gvcZ0O5_LWg"
      },
      "outputs": [],
      "source": [
        "explainer_LIME = LimeTabularExplainer(\n",
        "    training_data=X_RF_Combined_df.values,\n",
        "    training_labels=y_RF_Combined,\n",
        "    mode=\"classification\",\n",
        "    feature_names=X_RF_Combined_df.columns,\n",
        "    class_names=[\"Class 0\", \"Class 1\"],\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "instance_to_explain_RF = X_RF_Combined_df.iloc[10]\n",
        "\n",
        "def predict_combined(X):\n",
        "    return voting_classifier_RF_Combined.predict_proba(X)\n",
        "\n",
        "explanation_RF = explainer_LIME.explain_instance(\n",
        "    instance_to_explain_RF.values,\n",
        "    predict_combined,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "explanation_RF.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "explanation_RF.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xdxqb2nwvppC"
      },
      "source": [
        "# XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dc56803-dd8a-44f2-a8c8-c707346dceb9"
      },
      "source": [
        "## Cardiac Arrest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code tunes and evaluates an XGBoost model to predict cardiac arrest using Optuna and 5-fold stratified cross-validation. Optuna searches for the best hyperparameters by maximizing the average AUC-ROC across folds. Once the optimal parameters are found, the model is re-trained to report average performance metrics—accuracy, precision, recall, F1 score, and AUC-ROC. Finally, it plots the ROC curve to visualize the model’s ability to differentiate between patients with and without cardiac arrest."
      ],
      "metadata": {
        "id": "sAqj_9EnU8VU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVbpBIsn71sQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5YBgWYj7vj2"
      },
      "outputs": [],
      "source": [
        "def objective_XG_CA(trial):\n",
        "    param = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
        "        'verbosity': 0\n",
        "    }\n",
        "    kf_XG_CA = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "    fold_accuracies_XG_CA = []\n",
        "    fold_precisions_XG_CA = []\n",
        "    fold_recalls_XG_CA = []\n",
        "    fold_f1_scores_XG_CA = []\n",
        "    fold_roc_auc_XG_CA = []\n",
        "\n",
        "    for fold_idx, (train_index, test_index) in enumerate(kf_XG_CA.split(X_RF_CA, y_RF_CA)):\n",
        "        print(f\"Training fold {fold_idx + 1}\")\n",
        "\n",
        "        X_train_XG_CA, X_test_XG_CA = X_RF_CA.iloc[train_index], X_RF_CA.iloc[test_index]\n",
        "        y_train_XG_CA, y_test_XG_CA = y_RF_CA.iloc[train_index], y_RF_CA.iloc[test_index]\n",
        "\n",
        "        xgb_classifier_XG_CA = xgb.XGBClassifier(**param)\n",
        "\n",
        "        xgb_classifier_XG_CA.fit(X_train_XG_CA, y_train_XG_CA)\n",
        "\n",
        "        y_pred_XG_CA = xgb_classifier_XG_CA.predict(X_test_XG_CA)\n",
        "        y_pred_prob_XG_CA = xgb_classifier_XG_CA.predict_proba(X_test_XG_CA)[:, 1]  # Probabilities for AUC-ROC\n",
        "\n",
        "        accuracy_XG_CA = accuracy_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "        precision_XG_CA = precision_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "        recall_XG_CA = recall_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "        f1_XG_CA = f1_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "        roc_auc_XG_CA = roc_auc_score(y_test_XG_CA, y_pred_prob_XG_CA)\n",
        "\n",
        "        fold_accuracies_XG_CA.append(accuracy_XG_CA)\n",
        "        fold_precisions_XG_CA.append(precision_XG_CA)\n",
        "        fold_recalls_XG_CA.append(recall_XG_CA)\n",
        "        fold_f1_scores_XG_CA.append(f1_XG_CA)\n",
        "        fold_roc_auc_XG_CA.append(roc_auc_XG_CA)\n",
        "\n",
        "    avg_accuracy_XG_CA = np.mean(fold_accuracies_XG_CA)\n",
        "    avg_precision_XG_CA = np.mean(fold_precisions_XG_CA)\n",
        "    avg_recall_XG_CA = np.mean(fold_recalls_XG_CA)\n",
        "    avg_f1_XG_CA = np.mean(fold_f1_scores_XG_CA)\n",
        "    avg_roc_auc_XG_CA = np.mean(fold_roc_auc_XG_CA)\n",
        "\n",
        "    return avg_roc_auc_XG_CA\n",
        "\n",
        "study_XG_CA = optuna.create_study(direction='maximize')\n",
        "study_XG_CA.optimize(objective_XG_CA, n_trials=20)\n",
        "\n",
        "print(\"Best hyperparameters:\", study_XG_CA.best_params)\n",
        "\n",
        "best_params_XG_CA = study_XG_CA.best_params\n",
        "\n",
        "kf_XG_CA = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "fold_accuracies_XG_CA = []\n",
        "fold_precisions_XG_CA = []\n",
        "fold_recalls_XG_CA = []\n",
        "fold_f1_scores_XG_CA = []\n",
        "fold_roc_auc_XG_CA = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(kf_XG_CA.split(X_RF_CA, y_RF_CA)):\n",
        "    print(f\"Training fold {fold_idx + 1}\")\n",
        "\n",
        "    X_train_XG_CA, X_test_XG_CA = X_RF_CA.iloc[train_index], X_RF_CA.iloc[test_index]\n",
        "    y_train_XG_CA, y_test_XG_CA = y_RF_CA.iloc[train_index], y_RF_CA.iloc[test_index]\n",
        "\n",
        "    xgb_classifier_XG_CA = xgb.XGBClassifier(**best_params_XG_CA)\n",
        "\n",
        "    xgb_classifier_XG_CA.fit(X_train_XG_CA, y_train_XG_CA)\n",
        "\n",
        "    y_pred_XG_CA = xgb_classifier_XG_CA.predict(X_test_XG_CA)\n",
        "    y_pred_prob_XG_CA = xgb_classifier_XG_CA.predict_proba(X_test_XG_CA)[:, 1]\n",
        "\n",
        "    accuracy_XG_CA = accuracy_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "    precision_XG_CA = precision_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "    recall_XG_CA = recall_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "    f1_XG_CA = f1_score(y_test_XG_CA, y_pred_XG_CA)\n",
        "    roc_auc_XG_CA = roc_auc_score(y_test_XG_CA, y_pred_prob_XG_CA)\n",
        "\n",
        "    fold_accuracies_XG_CA.append(accuracy_XG_CA)\n",
        "    fold_precisions_XG_CA.append(precision_XG_CA)\n",
        "    fold_recalls_XG_CA.append(recall_XG_CA)\n",
        "    fold_f1_scores_XG_CA.append(f1_XG_CA)\n",
        "    fold_roc_auc_XG_CA.append(roc_auc_XG_CA)\n",
        "\n",
        "avg_accuracy_XG_CA = np.mean(fold_accuracies_XG_CA)\n",
        "avg_precision_XG_CA = np.mean(fold_precisions_XG_CA)\n",
        "avg_recall_XG_CA = np.mean(fold_recalls_XG_CA)\n",
        "avg_f1_XG_CA = np.mean(fold_f1_scores_XG_CA)\n",
        "avg_roc_auc_XG_CA = np.mean(fold_roc_auc_XG_CA)\n",
        "\n",
        "print(f\"\\nAverage accuracy from 5-fold cross-validation: {avg_accuracy_XG_CA:.4f}\")\n",
        "print(f\"Average precision: {avg_precision_XG_CA:.4f}\")\n",
        "print(f\"Average recall: {avg_recall_XG_CA:.4f}\")\n",
        "print(f\"Average F1 score: {avg_f1_XG_CA:.4f}\")\n",
        "print(f\"Average AUC-ROC: {avg_roc_auc_XG_CA:.4f}\")\n",
        "\n",
        "fpr_XG_CA, tpr_XG_CA, _ = roc_curve(y_test_XG_CA, y_pred_prob_XG_CA)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_XG_CA, tpr_XG_CA, label=f'AUC = {avg_roc_auc_XG_CA:.2f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - XGBClassifier (Cardiac Arrest)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a SHAP summary plot to interpret the XGBoost model trained to predict cardiac arrest. The plot ranks features by their overall importance and shows the direction and magnitude of their impact."
      ],
      "metadata": {
        "id": "6xPcuN9wW9xj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIAtpOKT8cBc"
      },
      "outputs": [],
      "source": [
        "plt.style.use('petroff10')\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "\n",
        "explainer_XG_CA = shap.Explainer(xgb_classifier_XG_CA, X_train_XG_CA)\n",
        "shap_values_XG_CA = explainer_XG_CA(X_test_XG_CA)\n",
        "\n",
        "feature_names = X_test_XG_CA.columns\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "shap.summary_plot(shap_values_XG_CA, X_test_XG_CA, feature_names=feature_names, plot_size=(18, 6), show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - XGBClassifier (Cardiac Arrest)\",\n",
        "             fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.0, bottom=0.1, left=0.1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a LIME explanation for a randomly selected patient from the test set, showing how the XGBoost model predicted their risk of cardiac arrest. It uses a LimeTabularExplainer to break down the prediction into feature-level contributions, helping to interpret which variables pushed the model toward or away from predicting an arrest."
      ],
      "metadata": {
        "id": "wcEIFJ03YSj7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xx2vbCSp8b2B"
      },
      "outputs": [],
      "source": [
        "lime_explainer_XG_CA = LimeTabularExplainer(\n",
        "    training_data=X_train_XG_CA.values,\n",
        "    feature_names=X_train_XG_CA.columns.tolist(),\n",
        "    class_names=['No Arrest', 'Arrest'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "idx_XG_CA = np.random.randint(0, X_test_XG_CA.shape[0])\n",
        "instance_to_explain = X_test_XG_CA.iloc[idx_XG_CA]\n",
        "\n",
        "lime_exp_XG_CA = lime_explainer_XG_CA.explain_instance(\n",
        "    instance_to_explain.values,\n",
        "    xgb_classifier_XG_CA.predict_proba\n",
        ")\n",
        "\n",
        "lime_exp_XG_CA.show_in_notebook()\n",
        "\n",
        "fig = lime_exp_XG_CA.as_pyplot_figure()\n",
        "plt.title(\"LIME Explanation - XGBClassifier (Cardiac Arrest)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a68aab1-e702-4c6a-a8bc-d4d2fc7cfceb"
      },
      "source": [
        "## CDMI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code tunes and evaluates an XGBoost model to predict myocardial infarction using Optuna and 5-fold stratified cross-validation. Optuna searches for the best hyperparameters by maximizing the average AUC-ROC across folds. Once the optimal parameters are found, the model is re-trained to report average performance metrics—accuracy, precision, recall, F1 score, and AUC-ROC. Finally, it plots the ROC curve to visualize the model’s ability to differentiate between patients with and without myocardial infarction."
      ],
      "metadata": {
        "id": "sIy7z-_UYZoj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtBXJ2HFYqMi"
      },
      "outputs": [],
      "source": [
        "def objective_XG_CDMI(trial):\n",
        "    param = {\n",
        "        'objective': 'binary:logistic',\n",
        "        'eval_metric': 'logloss',\n",
        "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-5, 1e-1),\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 50, 200),\n",
        "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
        "        'verbosity': 0\n",
        "    }\n",
        "    kf_XG_CDMI = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "    fold_accuracies_XG_CDMI = []\n",
        "    fold_precisions_XG_CDMI = []\n",
        "    fold_recalls_XG_CDMI = []\n",
        "    fold_f1_scores_XG_CDMI = []\n",
        "    fold_roc_auc_XG_CDMI = []\n",
        "\n",
        "\n",
        "    for fold_idx, (train_index, test_index) in enumerate(kf_XG_CDMI.split(X_RF_CDMI, y_RF_CDMI)):\n",
        "        print(f\"Training fold {fold_idx + 1}\")\n",
        "\n",
        "        X_train_XG_CDMI, X_test_XG_CDMI = X_RF_CDMI.iloc[train_index], X_RF_CDMI.iloc[test_index]\n",
        "        y_train_XG_CDMI, y_test_XG_CDMI = y_RF_CDMI.iloc[train_index], y_RF_CDMI.iloc[test_index]\n",
        "\n",
        "        xgb_classifier_XG_CDMI = xgb.XGBClassifier(**param)\n",
        "\n",
        "        xgb_classifier_XG_CDMI.fit(X_train_XG_CDMI, y_train_XG_CDMI)\n",
        "\n",
        "        y_pred_XG_CDMI = xgb_classifier_XG_CDMI.predict(X_test_XG_CDMI)\n",
        "        y_pred_prob_XG_CDMI = xgb_classifier_XG_CDMI.predict_proba(X_test_XG_CDMI)[:, 1]\n",
        "\n",
        "        accuracy_XG_CDMI = accuracy_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "        precision_XG_CDMI = precision_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "        recall_XG_CDMI = recall_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "        f1_XG_CDMI = f1_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "        roc_auc_XG_CDMI = roc_auc_score(y_test_XG_CDMI, y_pred_prob_XG_CDMI)\n",
        "\n",
        "        fold_accuracies_XG_CDMI.append(accuracy_XG_CDMI)\n",
        "        fold_precisions_XG_CDMI.append(precision_XG_CDMI)\n",
        "        fold_recalls_XG_CDMI.append(recall_XG_CDMI)\n",
        "        fold_f1_scores_XG_CDMI.append(f1_XG_CDMI)\n",
        "        fold_roc_auc_XG_CDMI.append(roc_auc_XG_CDMI)\n",
        "\n",
        "    avg_accuracy_XG_CDMI = np.mean(fold_accuracies_XG_CDMI)\n",
        "    avg_precision_XG_CDMI = np.mean(fold_precisions_XG_CDMI)\n",
        "    avg_recall_XG_CDMI = np.mean(fold_recalls_XG_CDMI)\n",
        "    avg_f1_XG_CDMI = np.mean(fold_f1_scores_XG_CDMI)\n",
        "    avg_roc_auc_XG_CDMI = np.mean(fold_roc_auc_XG_CDMI)\n",
        "\n",
        "    return avg_roc_auc_XG_CDMI\n",
        "\n",
        "study_XG_CDMI = optuna.create_study(direction='maximize')\n",
        "study_XG_CDMI.optimize(objective_XG_CDMI, n_trials=20)\n",
        "\n",
        "print(\"Best hyperparameters:\", study_XG_CDMI.best_params)\n",
        "\n",
        "best_params_XG_CDMI = study_XG_CDMI.best_params\n",
        "\n",
        "kf_XG_CDMI = StratifiedKFold(n_splits=5, shuffle=False)\n",
        "\n",
        "fold_accuracies_XG_CDMI = []\n",
        "fold_precisions_XG_CDMI = []\n",
        "fold_recalls_XG_CDMI = []\n",
        "fold_f1_scores_XG_CDMI = []\n",
        "fold_roc_auc_XG_CDMI = []\n",
        "\n",
        "for fold_idx, (train_index, test_index) in enumerate(kf_XG_CDMI.split(X_RF_CDMI, y_RF_CDMI)):\n",
        "    print(f\"Training fold {fold_idx + 1}\")\n",
        "\n",
        "    X_train_XG_CDMI, X_test_XG_CDMI = X_RF_CDMI.iloc[train_index], X_RF_CDMI.iloc[test_index]\n",
        "    y_train_XG_CDMI, y_test_XG_CDMI = y_RF_CDMI.iloc[train_index], y_RF_CDMI.iloc[test_index]\n",
        "\n",
        "    xgb_classifier_XG_CDMI = xgb.XGBClassifier(**best_params_XG_CDMI)\n",
        "\n",
        "    xgb_classifier_XG_CDMI.fit(X_train_XG_CDMI, y_train_XG_CDMI)\n",
        "\n",
        "    y_pred_XG_CDMI = xgb_classifier_XG_CDMI.predict(X_test_XG_CDMI)\n",
        "    y_pred_prob_XG_CDMI = xgb_classifier_XG_CDMI.predict_proba(X_test_XG_CDMI)[:, 1]\n",
        "\n",
        "    accuracy_XG_CDMI = accuracy_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "    precision_XG_CDMI = precision_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "    recall_XG_CDMI = recall_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "    f1_XG_CDMI = f1_score(y_test_XG_CDMI, y_pred_XG_CDMI)\n",
        "    roc_auc_XG_CDMI = roc_auc_score(y_test_XG_CDMI, y_pred_prob_XG_CDMI)\n",
        "\n",
        "    fold_accuracies_XG_CDMI.append(accuracy_XG_CDMI)\n",
        "    fold_precisions_XG_CDMI.append(precision_XG_CDMI)\n",
        "    fold_recalls_XG_CDMI.append(recall_XG_CDMI)\n",
        "    fold_f1_scores_XG_CDMI.append(f1_XG_CDMI)\n",
        "    fold_roc_auc_XG_CDMI.append(roc_auc_XG_CDMI)\n",
        "\n",
        "avg_accuracy_XG_CDMI = np.mean(fold_accuracies_XG_CDMI)\n",
        "avg_precision_XG_CDMI = np.mean(fold_precisions_XG_CDMI)\n",
        "avg_recall_XG_CDMI = np.mean(fold_recalls_XG_CDMI)\n",
        "avg_f1_XG_CDMI = np.mean(fold_f1_scores_XG_CDMI)\n",
        "avg_roc_auc_XG_CDMI = np.mean(fold_roc_auc_XG_CDMI)\n",
        "\n",
        "print(f\"\\nAverage accuracy from 5-fold cross-validation: {avg_accuracy_XG_CDMI:.4f}\")\n",
        "print(f\"Average precision: {avg_precision_XG_CDMI:.4f}\")\n",
        "print(f\"Average recall: {avg_recall_XG_CDMI:.4f}\")\n",
        "print(f\"Average F1 score: {avg_f1_XG_CDMI:.4f}\")\n",
        "print(f\"Average AUC-ROC: {avg_roc_auc_XG_CDMI:.4f}\")\n",
        "\n",
        "fpr_XG_CDMI, tpr_XG_CDMI, _ = roc_curve(y_test_XG_CDMI, y_pred_prob_XG_CDMI)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_XG_CDMI, tpr_XG_CDMI, label=f'AUC = {avg_roc_auc_XG_CDMI:.2f}')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.50)')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - XGBClassifier (CDMI)')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a SHAP summary plot to interpret the XGBoost model trained for predicting myocardial infarction. It uses SHAP values to highlight how each feature influenced model predictions on the test set, ranking them by importance. The plot provides insight into which clinical factors most strongly impact the myocardial infarction outcome."
      ],
      "metadata": {
        "id": "cTh8_SXcZPqL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phEAbW9WZDkN"
      },
      "outputs": [],
      "source": [
        "plt.style.use('petroff10')\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "\n",
        "explainer_XG_CDMI = shap.Explainer(xgb_classifier_XG_CDMI, X_train_XG_CDMI)\n",
        "shap_values_XG_CDMI = explainer_XG_CDMI(X_test_XG_CDMI)\n",
        "\n",
        "feature_names = X_test_XG_CDMI.columns\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "shap.summary_plot(shap_values_XG_CDMI, X_test_XG_CDMI, feature_names=feature_names, plot_size=(18, 6), show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - XGBClassifier (CDMI)\",\n",
        "             fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.0, bottom=0.1, left=0.1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to explain a single prediction made by the XGBoost model for myocardial infarction. A random instance from the test set is selected, and the LimeTabularExplainer breaks down how each feature contributed to the model's prediction for that case."
      ],
      "metadata": {
        "id": "OQUAZFXvZlEH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SDlBUOy_ZDVr"
      },
      "outputs": [],
      "source": [
        "lime_explainer_XG_CDMI = LimeTabularExplainer(\n",
        "    training_data=X_train_XG_CDMI.values,\n",
        "    feature_names=X_train_XG_CDMI.columns.tolist(),\n",
        "    class_names=['No CDMI', 'CDMI'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "idx_XG_CDMI = np.random.randint(0, X_test_XG_CDMI.shape[0])\n",
        "instance_to_explain = X_test_XG_CDMI.iloc[idx_XG_CDMI]\n",
        "\n",
        "lime_exp_XG_CDMI = lime_explainer_XG_CDMI.explain_instance(\n",
        "    instance_to_explain.values,\n",
        "    xgb_classifier_XG_CDMI.predict_proba\n",
        ")\n",
        "\n",
        "lime_exp_XG_CDMI.show_in_notebook()\n",
        "\n",
        "fig = lime_exp_XG_CDMI.as_pyplot_figure()\n",
        "plt.title(\"LIME Explanation - XGBClassifier (CDMI)\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcsTh8phYq1G"
      },
      "source": [
        "## Combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code resamples the two datasets to create balanced training data. Each dataset is upsampled to 1,000 instances using bootstrapping to ensure equal class representation. The resampled features and targets are then combined into a single dataset, and the feature names are preserved by converting the combined data into a properly labeled DataFrame. Then, an ensemble XGBoost model is built by combining the two optimized classifiers using a soft voting strategy. It trains the combined model on resampled, balanced data and evaluates its performance using accuracy, F1 score, recall, precision, and AUC. Finally, the ROC curve is plotted to visualize how well the ensemble model distinguishes between outcomes."
      ],
      "metadata": {
        "id": "_YsojTz0aBhO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlWB2_7aBR4L"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F-yjfxFZBJu8"
      },
      "outputs": [],
      "source": [
        "X_XG_CA_resampled, y_XG_CA_resampled = resample(dfca.drop(columns=['CDARREST']),\n",
        "                                                  dfca['CDARREST'],\n",
        "                                                  replace=True,\n",
        "                                                  n_samples=1000,\n",
        "                                                  random_state=42)\n",
        "\n",
        "X_XG_CDMI_resampled, y_XG_CDMI_resampled = resample(df1_hip_cleaned.drop(columns=['CDMI']),\n",
        "                                                    df1_hip_cleaned['CDMI'],\n",
        "                                                    replace=True,\n",
        "                                                    n_samples=1000,\n",
        "                                                    random_state=42)\n",
        "\n",
        "X_XG_Combined = pd.concat([X_XG_CA_resampled, X_XG_CDMI_resampled], axis=0)\n",
        "y_XG_Combined = pd.concat([y_XG_CA_resampled, y_XG_CDMI_resampled], axis=0)\n",
        "\n",
        "original_feature_names_XG = X_XG_CA_resampled.columns.tolist()\n",
        "\n",
        "X_XG_Combined_df = pd.DataFrame(X_XG_Combined, columns=original_feature_names_XG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyffIPb9cdrm"
      },
      "outputs": [],
      "source": [
        "xg_classifier_CA = xgb.XGBClassifier(**best_params_XG_CA)\n",
        "xg_classifier_CDMI = xgb.XGBClassifier(**best_params_XG_CDMI)\n",
        "\n",
        "original_feature_names_XG = X_XG_CA_resampled.columns.tolist()\n",
        "\n",
        "X_XG_Combined_df = pd.DataFrame(X_XG_Combined, columns=original_feature_names_XG)\n",
        "\n",
        "voting_classifier_XG_Combined = VotingClassifier(estimators=[\n",
        "    ('xg_CA', xg_classifier_CA),\n",
        "    ('xg_CDMI', xg_classifier_CDMI)\n",
        "], voting='soft')\n",
        "\n",
        "voting_classifier_XG_Combined.fit(X_XG_Combined, y_XG_Combined)\n",
        "\n",
        "y_pred_combined_labels_XG_Combined = voting_classifier_XG_Combined.predict(X_XG_Combined)\n",
        "\n",
        "accuracy_combined_XG_Combined = accuracy_score(y_XG_Combined, y_pred_combined_labels_XG_Combined)\n",
        "f1_combined_XG_Combined = f1_score(y_XG_Combined, y_pred_combined_labels_XG_Combined)\n",
        "recall_combined_XG_Combined = recall_score(y_XG_Combined, y_pred_combined_labels_XG_Combined)\n",
        "precision_combined_XG_Combined = precision_score(y_XG_Combined, y_pred_combined_labels_XG_Combined)\n",
        "\n",
        "y_pred_combined_prob_XG_Combined = voting_classifier_XG_Combined.predict_proba(X_XG_Combined)[:, 1]\n",
        "fpr_combined_XG_Combined, tpr_combined_XG_Combined, _ = roc_curve(y_XG_Combined, y_pred_combined_prob_XG_Combined)\n",
        "roc_auc_combined_XG_Combined = auc(fpr_combined_XG_Combined, tpr_combined_XG_Combined)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_combined_XG_Combined, tpr_combined_XG_Combined, color='red', lw=2, label=f'Combined Model (AUC = {roc_auc_combined_XG_Combined:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Combined XGBoost Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'Combined Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined_XG_Combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined_XG_Combined:.4f}')\n",
        "print(f'Recall: {recall_combined_XG_Combined:.4f}')\n",
        "print(f'Precision: {precision_combined_XG_Combined:.4f}')\n",
        "print(f'AUC Score: {roc_auc_combined_XG_Combined:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code aligns the feature sets from two datasets by adding missing features (filled with zeros) so both models can interpret the same unified dataset. It then calculates SHAP values for each model on the combined data and averages them to create a unified SHAP explanation. The final SHAP summary plot visualizes the overall feature importance across both XGBoost models."
      ],
      "metadata": {
        "id": "qc-DQTcAcPzo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GdHqnwrNc0F"
      },
      "outputs": [],
      "source": [
        "features_CA = set(X_XG_CA_resampled.columns)\n",
        "features_CDMI = set(X_XG_CDMI_resampled.columns)\n",
        "\n",
        "extra_features_CA = features_CA - features_CDMI\n",
        "extra_features_CDMI = features_CDMI - features_CA\n",
        "\n",
        "for feature in extra_features_CA:\n",
        "    X_XG_Combined_df[feature] = 0\n",
        "for feature in extra_features_CDMI:\n",
        "    X_XG_Combined_df[feature] = 0\n",
        "\n",
        "X_XG_Combined_df = X_XG_Combined_df[sorted(X_XG_Combined_df.columns)]\n",
        "\n",
        "explainer_XG_CA = shap.TreeExplainer(xg_classifier_CA)\n",
        "explainer_XG_CDMI = shap.TreeExplainer(xg_classifier_CDMI)\n",
        "\n",
        "shap_values_XG_CA = explainer_XG_CA.shap_values(X_XG_Combined_df)\n",
        "shap_values_XG_CDMI = explainer_XG_CDMI.shap_values(X_XG_Combined_df)\n",
        "\n",
        "shap_values_XG_Combined = (shap_values_XG_CA + shap_values_XG_CDMI) / 2\n",
        "\n",
        "assert shap_values_XG_Combined.shape[1] == X_XG_Combined_df.shape[1], \\\n",
        "    f\"Shape mismatch: SHAP values ({shap_values_XG_Combined.shape[1]} features) and data ({X_XG_Combined_df.shape[1]} features)\"\n",
        "\n",
        "shap.summary_plot(shap_values_XG_Combined, X_XG_Combined_df, title=\"SHAP Summary Plot - Combined XGBoost Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Tlim4R1EPne"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uLuXaixE0Kc"
      },
      "source": [
        "## Cardiac Arrest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code develops a logistic regression model to predict cardiac arrest, using Optuna to fine-tune hyperparameters through 5-fold stratified cross-validation. Then, the final model is trained on the full dataset and evaluated. Key performance metrics (AUC-ROC, accuracy, F1, recall, and precision) are calculated, and the ROC curve is plotted to assess how well the model distinguishes between positive and negative outcomes."
      ],
      "metadata": {
        "id": "CuJZ8fLPclmT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cruKWGX_SKB8"
      },
      "outputs": [],
      "source": [
        "X_LR_CA = dfca.drop(columns=['CDARREST'])\n",
        "y_LR_CA = dfca['CDARREST']\n",
        "\n",
        "scaler_LR_CA = StandardScaler()\n",
        "X_scaled_LR_CA = scaler_LR_CA.fit_transform(X_LR_CA)\n",
        "\n",
        "kf_LR_CA = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def lr_ca_objective(trial):\n",
        "    C = trial.suggest_loguniform('C', 1e-4, 10)\n",
        "    solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'newton-cg'])\n",
        "    auc_scores = []\n",
        "\n",
        "    for train_index, test_index in kf_LR_CA.split(X_scaled_LR_CA, y_LR_CA):\n",
        "        X_train_LR_CA, X_test_LR_CA = X_scaled_LR_CA[train_index], X_scaled_LR_CA[test_index]\n",
        "        y_train_LR_CA, y_test_LR_CA = y_LR_CA[train_index], y_LR_CA[test_index]\n",
        "\n",
        "        model = LogisticRegression(C=C, solver=solver, max_iter=1000, random_state=42)\n",
        "        model.fit(X_train_LR_CA, y_train_LR_CA)\n",
        "\n",
        "        y_prob_LR_CA = model.predict_proba(X_test_LR_CA)[:, 1]\n",
        "        auc_score = roc_auc_score(y_test_LR_CA, y_prob_LR_CA)\n",
        "        auc_scores.append(auc_score)\n",
        "\n",
        "    return np.mean(auc_scores)\n",
        "\n",
        "study_LR_CA = optuna.create_study(direction=\"maximize\")\n",
        "study_LR_CA.optimize(lr_ca_objective, n_trials=50)\n",
        "\n",
        "best_params_LR_CA = study_LR_CA.best_params\n",
        "print(\"Best hyperparameters for Logistic Regression:\", best_params_LR_CA)\n",
        "\n",
        "log_reg_LR_CA = LogisticRegression(**best_params_LR_CA, max_iter=1000, random_state=42)\n",
        "\n",
        "log_reg_LR_CA.fit(X_scaled_LR_CA, y_LR_CA)\n",
        "\n",
        "y_pred_LR_CA = log_reg_LR_CA.predict(X_scaled_LR_CA)\n",
        "y_prob_LR_CA = log_reg_LR_CA.predict_proba(X_scaled_LR_CA)[:, 1]\n",
        "\n",
        "LR_CA_fpr, LR_CA_tpr, _ = roc_curve(y_LR_CA, y_prob_LR_CA)\n",
        "roc_auc_LR_CA = roc_auc_score(y_LR_CA, y_prob_LR_CA)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(LR_CA_fpr, LR_CA_tpr, color='blue', label=f'AUC = {roc_auc_LR_CA:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.title('Logistic Regression (Cardiac Arrest) ROC Curve', fontsize=16)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_LR_CA, y_pred_LR_CA))\n",
        "print(\"AUC-ROC:\", roc_auc_LR_CA)\n",
        "accuracy_LR_CA = accuracy_score(y_LR_CA, y_pred_LR_CA)\n",
        "print(f\"Overall Accuracy (Logistic Regression (Cardiac Arrest)): {accuracy_LR_CA:.4f}\")\n",
        "f1_LR_CA = f1_score(y_LR_CA, y_pred_LR_CA)\n",
        "print(f\"F1 Score (Logistic Regression (Cardiac Arrest)): {f1_LR_CA:.4f}\")\n",
        "recall_LR_CA = recall_score(y_LR_CA, y_pred_LR_CA)\n",
        "print(f\"Recall Score (Logistic Regression (Cardiac Arrest)): {recall_LR_CA:.4f}\")\n",
        "precision_LR_CA = recall_score(y_LR_CA, y_pred_LR_CA)\n",
        "print(f\"Precision Score (Logistic Regression (Cardiac Arrest)): {precision_LR_CA:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code visualizes the feature contributions of a logistic regression model predicting cardiac arrest using SHAP. After computing SHAP values across the entire dataset, a summary plot is generated to rank features by their impact on the model's predictions."
      ],
      "metadata": {
        "id": "GWGPx4dcdM9k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VuVv2SOSVn8"
      },
      "outputs": [],
      "source": [
        "explainer_LR_CA = shap.Explainer(log_reg_LR_CA, X_scaled_LR_CA)\n",
        "shap_values_LR_CA = explainer_LR_CA(X_scaled_LR_CA)\n",
        "\n",
        "feature_names = X_LR_CA.columns\n",
        "\n",
        "plt.figure(figsize=(22, 10))\n",
        "\n",
        "shap.summary_plot(shap_values_LR_CA, X_scaled_LR_CA, feature_names=feature_names, show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - Logistic Regression (Cardiac Arrest)\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.3, bottom=0.1, left=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to explain how the logistic regression model predicted cardiac arrest for a randomly selected patient. It applies LimeTabularExplainer to break down the model's prediction into individual feature contributions, showing which variables pushed the prediction toward or away from cardiac arrest."
      ],
      "metadata": {
        "id": "Zu5OXFsYdpQx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZwuQ5LVWvNT"
      },
      "outputs": [],
      "source": [
        "explainer_LR_CA = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_scaled_LR_CA,\n",
        "    feature_names=X_LR_CA.columns,\n",
        "    class_names=['No CPR', 'CPR'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "i = np.random.randint(0, X_scaled_LR_CA.shape[0])\n",
        "sample_instance = X_scaled_LR_CA[i]\n",
        "\n",
        "explanation_LR_CA = explainer_LR_CA.explain_instance(\n",
        "    sample_instance,\n",
        "    log_reg_LR_CA.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "explanation_LR_CA.show_in_notebook()\n",
        "\n",
        "fig = explanation_LR_CA.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vao6s9FIE3iI"
      },
      "source": [
        "## CDMI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code develops a logistic regression model to predict myocardial infarction using Optuna to fine-tune hyperparameters through 5-fold stratified cross-validation. Then, the final model is trained on the full dataset and evaluated. Key performance metrics (AUC-ROC, accuracy, F1, recall, and precision) are calculated, and the ROC curve is plotted to assess how well the model distinguishes between positive and negative outcomes."
      ],
      "metadata": {
        "id": "7ZeNT5xxd7m9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d8u7wRkbFgQf"
      },
      "outputs": [],
      "source": [
        "X_LR_CDMI = dfcdmi.drop(columns=['CDMI'])\n",
        "y_LR_CDMI = dfcdmi['CDMI']\n",
        "\n",
        "scaler_LR_CDMI = StandardScaler()\n",
        "X_scaled_LR_CDMI = scaler_LR_CDMI.fit_transform(X_LR_CDMI)\n",
        "\n",
        "kf_LR_CDMI = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def lr_cdmi_objective(trial):\n",
        "    C = trial.suggest_loguniform('C', 1e-4, 10)\n",
        "    solver = trial.suggest_categorical('solver', ['liblinear', 'lbfgs', 'newton-cg'])\n",
        "    auc_scores = []\n",
        "\n",
        "    for train_index, test_index in kf_LR_CDMI.split(X_scaled_LR_CDMI, y_LR_CDMI):\n",
        "        X_train_LR_CDMI, X_test_LR_CDMI = X_scaled_LR_CDMI[train_index], X_scaled_LR_CDMI[test_index]\n",
        "        y_train_LR_CDMI, y_test_LR_CDMI = y_LR_CDMI[train_index], y_LR_CDMI[test_index]\n",
        "\n",
        "        model = LogisticRegression(C=C, solver=solver, max_iter=1000, random_state=42)\n",
        "        model.fit(X_train_LR_CDMI, y_train_LR_CDMI)\n",
        "\n",
        "        y_prob_LR_CDMI = model.predict_proba(X_test_LR_CDMI)[:, 1]\n",
        "        auc_score = roc_auc_score(y_test_LR_CDMI, y_prob_LR_CDMI)\n",
        "        auc_scores.append(auc_score)\n",
        "\n",
        "    return np.mean(auc_scores)\n",
        "\n",
        "study_LR_CDMI = optuna.create_study(direction=\"maximize\")\n",
        "study_LR_CDMI.optimize(lr_cdmi_objective, n_trials=50)\n",
        "\n",
        "best_params_LR_CDMI = study_LR_CDMI.best_params\n",
        "print(\"Best hyperparameters for Logistic Regression:\", best_params_LR_CDMI)\n",
        "\n",
        "log_reg_LR_CDMI = LogisticRegression(**best_params_LR_CDMI, max_iter=1000, random_state=42)\n",
        "\n",
        "log_reg_LR_CDMI.fit(X_scaled_LR_CDMI, y_LR_CDMI)\n",
        "\n",
        "y_pred_LR_CDMI = log_reg_LR_CDMI.predict(X_scaled_LR_CDMI)\n",
        "y_prob_LR_CDMI = log_reg_LR_CDMI.predict_proba(X_scaled_LR_CDMI)[:, 1]\n",
        "\n",
        "_R_CDMI_fpr, LR_CDMI_tpr, _ = roc_curve(y_LR_CDMI, y_prob_LR_CDMI)\n",
        "roc_auc_LR_CDMI = roc_auc_score(y_LR_CDMI, y_prob_LR_CDMI)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(_R_CDMI_fpr, LR_CDMI_tpr, color='blue', label=f'AUC = {roc_auc_LR_CDMI:.2f}')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.title('Logistic Regression (CDMI) ROC Curve', fontsize=16)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "print(classification_report(y_LR_CDMI, y_pred_LR_CDMI))\n",
        "print(\"AUC-ROC:\", roc_auc_LR_CDMI)\n",
        "accuracy_LR_CDMI = accuracy_score(y_LR_CDMI, y_pred_LR_CDMI)\n",
        "print(f\"Overall Accuracy (Logistic Regression (CDMI)): {accuracy_LR_CDMI:.4f}\")\n",
        "f1_LR_CDMI = f1_score(y_LR_CDMI, y_pred_LR_CDMI)\n",
        "print(f\"F1 Score (Logistic Regression (CDMI)): {f1_LR_CDMI:.4f}\")\n",
        "recall_LR_CDMI = recall_score(y_LR_CDMI, y_pred_LR_CDMI)\n",
        "print(f\"Recall Score (Logistic Regression (CDMI)): {recall_LR_CDMI:.4f}\")\n",
        "precision_LR_CDMI = precision_score(y_LR_CDMI, y_pred_LR_CDMI)\n",
        "print(f\"Precision Score (Logistic Regression (CDMI)): {precision_LR_CDMI:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code produces a SHAP summary plot to interpret the logistic regression model trained to predict CDMI. It calculates SHAP values to quantify how each feature influenced the model’s output across the entire dataset."
      ],
      "metadata": {
        "id": "ZmPd2IzKeOOa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0k-i7ceSptZ"
      },
      "outputs": [],
      "source": [
        "explainer_LR_CDMI = shap.Explainer(log_reg_LR_CDMI, X_scaled_LR_CDMI)\n",
        "shap_values_LR_CDMI = explainer_LR_CDMI(X_scaled_LR_CDMI)\n",
        "\n",
        "feature_names = X_LR_CDMI.columns\n",
        "\n",
        "plt.figure(figsize=(22, 10))\n",
        "\n",
        "shap.summary_plot(shap_values_LR_CDMI, X_scaled_LR_CDMI, feature_names=feature_names, show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - Logistic Regression (CDMI)\", fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.3, bottom=0.1, left=0.2)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to interpret a logistic regression model’s prediction for CDMI on a single, randomly selected patient. It explains how individual features contributed to the model's output using the predict_proba function."
      ],
      "metadata": {
        "id": "eJjufAh3ejfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRqAtE28SVcC"
      },
      "outputs": [],
      "source": [
        "explainer_LR_CDMI = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_scaled_LR_CDMI,\n",
        "    feature_names=X_LR_CDMI.columns,\n",
        "    class_names=['No CDMI', 'CDMI'],\n",
        "    mode='classification'\n",
        ")\n",
        "\n",
        "i = np.random.randint(0, X_scaled_LR_CDMI.shape[0])\n",
        "sample_instance = X_scaled_LR_CDMI[i]\n",
        "\n",
        "explanation_LR_CDMI = explainer_LR_CDMI.explain_instance(\n",
        "    sample_instance,\n",
        "    log_reg_LR_CDMI.predict_proba,\n",
        "    num_features=10\n",
        ")\n",
        "explanation_LR_CDMI.show_in_notebook()\n",
        "\n",
        "fig = explanation_LR_CDMI.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LRdhJvQKSTFk"
      },
      "source": [
        "## Combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a combined logistic regression model using soft voting to integrate predictions from the two models. It merges the feature sets and labels, fits the ensemble model on the full dataset, and evaluates its performance using standard metrics: accuracy, F1 score, recall, precision, and AUC-ROC. The ROC curve is plotted to visualize the model’s ability to distinguish between positive and negative outcomes."
      ],
      "metadata": {
        "id": "iF0sNGcIe8fX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcHQTwwxSUme"
      },
      "outputs": [],
      "source": [
        "X_LR_CA_resampled = dfca.drop('CDARREST', axis=1)\n",
        "y_LR_CA_resampled = dfca['CDARREST']\n",
        "\n",
        "X_LR_CDMI_resampled = df1_hip_cleaned.drop('CDMI', axis=1)\n",
        "y_LR_CDMI_resampled = df1_hip_cleaned['CDMI']\n",
        "\n",
        "X_LR_Combined = np.concatenate((X_LR_CA_resampled, X_LR_CDMI_resampled), axis=0)\n",
        "y_LR_Combined = np.concatenate((y_LR_CA_resampled, y_LR_CDMI_resampled), axis=0)\n",
        "\n",
        "original_feature_names = X_LR_CA_resampled.columns.tolist()\n",
        "\n",
        "X_LR_Combined_df = pd.DataFrame(X_LR_Combined, columns=original_feature_names)\n",
        "\n",
        "log_reg_CA = LogisticRegression(max_iter=1000)\n",
        "log_reg_CDMI = LogisticRegression(max_iter=1000)\n",
        "\n",
        "voting_classifier_LR_Combined = VotingClassifier(estimators=[\n",
        "    ('log_reg_CA', log_reg_CA),\n",
        "    ('log_reg_CDMI', log_reg_CDMI)\n",
        "], voting='soft')\n",
        "\n",
        "voting_classifier_LR_Combined.fit(X_LR_Combined, y_LR_Combined)\n",
        "\n",
        "y_pred_combined_labels = voting_classifier_LR_Combined.predict(X_LR_Combined)\n",
        "\n",
        "accuracy_combined = accuracy_score(y_LR_Combined, y_pred_combined_labels)\n",
        "f1_combined = f1_score(y_LR_Combined, y_pred_combined_labels)\n",
        "recall_combined = recall_score(y_LR_Combined, y_pred_combined_labels)\n",
        "precision_combined = precision_score(y_LR_Combined, y_pred_combined_labels)\n",
        "\n",
        "y_pred_combined_prob = voting_classifier_LR_Combined.predict_proba(X_LR_Combined)[:, 1]\n",
        "fpr_combined, tpr_combined, _ = roc_curve(y_LR_Combined, y_pred_combined_prob)\n",
        "roc_auc_combined = auc(fpr_combined, tpr_combined)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr_combined, tpr_combined, color='red', lw=2, label=f'Combined Model (AUC = {roc_auc_combined:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Combined Logistic Regression Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "print(f'Combined Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined:.4f}')\n",
        "print(f'Recall: {recall_combined:.4f}')\n",
        "print(f'AUC Score: {roc_auc_combined:.4f}')\n",
        "print(f'Precision: {precision_combined:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiQ2CaXfWfLn"
      },
      "outputs": [],
      "source": [
        "log_reg_CA.fit(X_LR_CA_resampled, y_RF_CA_resampled)\n",
        "log_reg_CDMI.fit(X_LR_CDMI_resampled, y_RF_CDMI_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a combined SHAP summary plot for the two logistic regression models. Each model is trained separately, and SHAP values are calculated across a unified dataset. The SHAP values from both models are then averaged to reflect the joint influence of features in the ensemble setting. The final summary plot visualizes which variables consistently impact predictions across both models."
      ],
      "metadata": {
        "id": "bbEB9oWCgVEz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZL6tgmpUJN0"
      },
      "outputs": [],
      "source": [
        "log_reg_CA.fit(X_LR_CA_resampled, y_RF_CA_resampled)\n",
        "log_reg_CDMI.fit(X_LR_CDMI_resampled, y_RF_CDMI_resampled)\n",
        "\n",
        "X_LR_Combined_df = pd.DataFrame(X_LR_Combined, columns=X_LR_CA_resampled.columns)\n",
        "\n",
        "explainer_LR_CA = shap.LinearExplainer(log_reg_CA, X_LR_Combined_df)\n",
        "explainer_LR_CDMI = shap.LinearExplainer(log_reg_CDMI, X_LR_Combined_df)\n",
        "\n",
        "shap_values_LR_CA = explainer_LR_CA.shap_values(X_LR_Combined_df)\n",
        "shap_values_LR_CDMI = explainer_LR_CDMI.shap_values(X_LR_Combined_df)\n",
        "\n",
        "shap_values_Combined = (shap_values_LR_CA + shap_values_LR_CDMI) / 2\n",
        "\n",
        "assert shap_values_Combined.shape[1] == X_LR_Combined_df.shape[1], \\\n",
        "    f\"Shape mismatch: SHAP values ({shap_values_Combined.shape[1]} features) and data ({X_LR_Combined_df.shape[1]} features)\"\n",
        "\n",
        "shap.summary_plot(shap_values_Combined, X_LR_Combined_df, plot_type=\"dot\")\n",
        "\n",
        "plt.suptitle('SHAP Summary Plot - Combined Logistic Regression Model', fontsize=16)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rhZBYVvXVNL"
      },
      "outputs": [],
      "source": [
        "log_reg_CA.fit(X_LR_CA_resampled, y_RF_CA_resampled)\n",
        "log_reg_CDMI.fit(X_LR_CDMI_resampled, y_RF_CDMI_resampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a LIME explanation for a combined logistic regression model that predicts cardiac arrest or myocardial infarction. It includes several assertion checks to ensure that the data and feature dimensions align correctly, preventing errors during explanation. Once validated, LIME is used to explain a single instance (the first row of the combined dataset), showing how individual features contributed to the model’s prediction."
      ],
      "metadata": {
        "id": "48wQtdd7glDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0RiDmGRXKcd"
      },
      "outputs": [],
      "source": [
        "assert X_LR_Combined.shape[1] == len(X_LR_Combined_df.columns), \\\n",
        "    f\"Feature count mismatch: Data has {X_LR_Combined.shape[1]} features, but {len(X_LR_Combined_df.columns)} feature names were provided.\"\n",
        "\n",
        "assert all([col in X_LR_Combined_df.columns for col in X_LR_Combined_df.columns.tolist()]), \\\n",
        "    f\"Mismatch in feature names: Some feature names are missing in the DataFrame.\"\n",
        "\n",
        "assert X_LR_Combined.shape[1] == X_LR_Combined_df.shape[1], \\\n",
        "    f\"Feature count mismatch: Model data has {X_LR_Combined.shape[1]} features, but DataFrame has {X_LR_Combined_df.shape[1]} features.\"\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_LR_Combined,\n",
        "    training_labels=y_LR_Combined,\n",
        "    feature_names=X_LR_Combined_df.columns.tolist(),\n",
        "    class_names=['No Arrest', 'Arrest'],\n",
        "    mode='classification',\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "instance_to_explain = X_LR_Combined[0]\n",
        "\n",
        "assert instance_to_explain.shape[0] == X_LR_Combined.shape[1], \\\n",
        "    f\"Instance shape mismatch: Instance has {instance_to_explain.shape[0]} features, but expected {X_LR_Combined.shape[1]}.\"\n",
        "\n",
        "prediction_to_explain = voting_classifier_LR_Combined.predict_proba([instance_to_explain])\n",
        "\n",
        "assert prediction_to_explain.shape[1] == 2, \\\n",
        "    f\"Prediction shape mismatch: Expected 2 classes, but got {prediction_to_explain.shape[1]} classes.\"\n",
        "\n",
        "explanation = explainer.explain_instance(instance_to_explain, voting_classifier_LR_Combined.predict_proba, num_features=10)\n",
        "\n",
        "explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "fig = explanation.as_pyplot_figure()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nH1KzZMzFrSy"
      },
      "source": [
        "# Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlgtF6cuKtCP"
      },
      "source": [
        "## Cardiac Arrest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code performs 5-fold cross-validation to evaluate a neural network model (model_NN_CA) designed to predict cardiac arrest. It trains and tests the model on different subsets of the data (X_NN_CA, y_NN_CA) while calculating key performance metrics for each fold: AUC-ROC, accuracy, precision, recall, and F1 score. The function returns the averaged ROC curve data and mean evaluation metrics, making it useful for both visual and quantitative model performance comparison."
      ],
      "metadata": {
        "id": "4-prQ6zChLA4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uzvTFivuBVs"
      },
      "outputs": [],
      "source": [
        "def kfold_cross_validation_NN_CA(X_NN_CA, y_NN_CA, model_NN_CA, n_splits=5):\n",
        "    kf_NN_CA = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_NN_CA = []\n",
        "    accuracy_scores_NN_CA = []\n",
        "    precision_scores_NN_CA = []\n",
        "    recall_scores_NN_CA = []\n",
        "    f1_scores_NN_CA = []\n",
        "\n",
        "    fpr_list_NN_CA, tpr_list_NN_CA = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CA.split(X_NN_CA):\n",
        "        X_train_NN_CA, X_test_NN_CA = X_NN_CA[train_index], X_NN_CA[test_index]\n",
        "        y_train_NN_CA, y_test_NN_CA = y_NN_CA[train_index], y_NN_CA[test_index]\n",
        "\n",
        "        model_NN_CA.fit(X_train_NN_CA, y_train_NN_CA, epochs=20, batch_size=32, class_weight=class_weight_NN_CA, verbose=0)\n",
        "        y_prob_NN_CA = model_NN_CA.predict(X_test_NN_CA).flatten()\n",
        "        y_pred_NN_CA = (y_prob_NN_CA > 0.5).astype(int)\n",
        "\n",
        "        fpr_NN_CA, tpr_NN_CA, _ = roc_curve(y_test_NN_CA, y_prob_NN_CA)\n",
        "        roc_auc_NN_CA = roc_auc_score(y_test_NN_CA, y_prob_NN_CA)\n",
        "\n",
        "        fpr_list_NN_CA.append(fpr_NN_CA)\n",
        "        tpr_list_NN_CA.append(tpr_NN_CA)\n",
        "        roc_auc_scores_NN_CA.append(roc_auc_NN_CA)\n",
        "\n",
        "        accuracy_NN_CA = accuracy_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        precision_NN_CA = precision_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        recall_NN_CA = recall_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        f1_NN_CA = f1_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "\n",
        "        accuracy_scores_NN_CA.append(accuracy_NN_CA)\n",
        "        precision_scores_NN_CA.append(precision_NN_CA)\n",
        "        recall_scores_NN_CA.append(recall_NN_CA)\n",
        "        f1_scores_NN_CA.append(f1_NN_CA)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CA:.4f} | Accuracy: {accuracy_NN_CA:.4f} | Precision: {precision_NN_CA:.4f} | Recall: {recall_NN_CA:.4f} | F1: {f1_NN_CA:.4f}\")\n",
        "\n",
        "    mean_fpr_NN_CA = np.linspace(0, 1, 100)\n",
        "    mean_tpr_NN_CA = np.zeros_like(mean_fpr_NN_CA)\n",
        "\n",
        "    for fpr_NN_CA, tpr_NN_CA in zip(fpr_list_NN_CA, tpr_list_NN_CA):\n",
        "        mean_tpr_NN_CA += np.interp(mean_fpr_NN_CA, fpr_NN_CA, tpr_NN_CA)\n",
        "\n",
        "    mean_tpr_NN_CA /= len(fpr_list_NN_CA)\n",
        "    mean_roc_auc_NN_CA = np.mean(roc_auc_scores_NN_CA)\n",
        "\n",
        "    mean_accuracy_NN_CA = np.mean(accuracy_scores_NN_CA)\n",
        "    mean_precision_NN_CA = np.mean(precision_scores_NN_CA)\n",
        "    mean_recall_NN_CA = np.mean(recall_scores_NN_CA)\n",
        "    mean_f1_NN_CA = np.mean(f1_scores_NN_CA)\n",
        "\n",
        "    print(f\"\\nAverage metrics across all folds:\")\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CA:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CA:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CA:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CA:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CA:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CA, mean_tpr_NN_CA, mean_roc_auc_NN_CA, mean_accuracy_NN_CA, mean_precision_NN_CA, mean_recall_NN_CA, mean_f1_NN_CA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZ1RMAvtud7H"
      },
      "outputs": [],
      "source": [
        "# Assume 'CA' is the target variable for your model, replace with the actual column name if necessary\n",
        "X_NN_CA = dfca.drop('CDARREST', axis=1).values  # Features (excluding the target column 'CA')\n",
        "y_NN_CA = dfca['CDARREST'].values  # Target variable (e.g., 'CA')\n",
        "\n",
        "# Scale the features\n",
        "scaler_NN_CA = StandardScaler()\n",
        "X_scaled_NN_CA = scaler_NN_CA.fit_transform(X_NN_CA)  # Fit and transform on the entire dataset\n",
        "\n",
        "# Define the model creation function\n",
        "def build_nn_CA_Model(input_dim, best_params_NN_CA):\n",
        "    model_NN_CA = Sequential()\n",
        "    model_NN_CA.add(Dense(best_params_NN_CA['units_1'], input_dim=input_dim, activation='relu'))\n",
        "    model_NN_CA.add(Dropout(best_params_NN_CA['dropout_1']))\n",
        "    model_NN_CA.add(Dense(best_params_NN_CA['units_2'], activation='relu'))\n",
        "    model_NN_CA.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
        "\n",
        "    model_NN_CA.compile(optimizer=Adam(learning_rate=best_params_NN_CA['learning_rate']),\n",
        "                          loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model_NN_CA\n",
        "\n",
        "# Define the objective function for hyperparameter optimization\n",
        "def optimize_nn_CA_with_optuna():\n",
        "    def objective(trial):\n",
        "        units_1 = trial.suggest_int('units_1', 64, 256)\n",
        "        dropout_1 = trial.suggest_float('dropout_1', 0.2, 0.5)\n",
        "        units_2 = trial.suggest_int('units_2', 32, 128)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "\n",
        "        model_NN_CA = build_nn_CA_Model(X_scaled_NN_CA.shape[1],\n",
        "                                            {'units_1': units_1, 'dropout_1': dropout_1, 'units_2': units_2, 'learning_rate': learning_rate})\n",
        "\n",
        "        model_NN_CA.fit(X_scaled_NN_CA, y_NN_CA, epochs=10, batch_size=32, verbose=0)\n",
        "        y_prob_NN_CA = model_NN_CA.predict(X_scaled_NN_CA).flatten()\n",
        "        roc_auc_NN_CA = roc_auc_score(y_NN_CA, y_prob_NN_CA)\n",
        "\n",
        "        return roc_auc_NN_CA\n",
        "\n",
        "    study_NN_CA = optuna.create_study(direction='maximize')\n",
        "    study_NN_CA.optimize(objective, n_trials=50)\n",
        "\n",
        "    return study_NN_CA.best_params\n",
        "\n",
        "# K-fold Cross-validation with additional metrics (Random Split)\n",
        "def kfold_cross_validation_NN_CA(X_NN_CA, y_NN_CA, model_NN_CA, n_splits=5):\n",
        "    kf_NN_CA = KFold(n_splits=n_splits, shuffle=True, random_state=42)  # Random split (shuffle=True)\n",
        "\n",
        "    roc_auc_scores_NN_CA = []\n",
        "    accuracy_scores_NN_CA = []\n",
        "    precision_scores_NN_CA = []\n",
        "    recall_scores_NN_CA = []\n",
        "    f1_scores_NN_CA = []\n",
        "\n",
        "    fpr_list_NN_CA, tpr_list_NN_CA = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CA.split(X_NN_CA):\n",
        "        X_train_NN_CA, X_test_NN_CA = X_NN_CA[train_index], X_NN_CA[test_index]\n",
        "        y_train_NN_CA, y_test_NN_CA = y_NN_CA[train_index], y_NN_CA[test_index]\n",
        "\n",
        "        model_NN_CA.fit(X_train_NN_CA, y_train_NN_CA, epochs=20, batch_size=32, verbose=0)\n",
        "        y_prob_NN_CA = model_NN_CA.predict(X_test_NN_CA).flatten()\n",
        "        y_pred_NN_CA = (y_prob_NN_CA > 0.5).astype(int)\n",
        "\n",
        "        # Collect ROC metrics\n",
        "        fpr_NN_CA, tpr_NN_CA, _ = roc_curve(y_test_NN_CA, y_prob_NN_CA)\n",
        "        roc_auc_NN_CA = roc_auc_score(y_test_NN_CA, y_prob_NN_CA)\n",
        "\n",
        "        fpr_list_NN_CA.append(fpr_NN_CA)\n",
        "        tpr_list_NN_CA.append(tpr_NN_CA)\n",
        "        roc_auc_scores_NN_CA.append(roc_auc_NN_CA)\n",
        "\n",
        "        # Evaluation metrics\n",
        "        accuracy_NN_CA = accuracy_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        precision_NN_CA = precision_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        recall_NN_CA = recall_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "        f1_NN_CA = f1_score(y_test_NN_CA, y_pred_NN_CA)\n",
        "\n",
        "        accuracy_scores_NN_CA.append(accuracy_NN_CA)\n",
        "        precision_scores_NN_CA.append(precision_NN_CA)\n",
        "        recall_scores_NN_CA.append(recall_NN_CA)\n",
        "        f1_scores_NN_CA.append(f1_NN_CA)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CA:.4f} | Accuracy: {accuracy_NN_CA:.4f} | Precision: {precision_NN_CA:.4f} | Recall: {recall_NN_CA:.4f} | F1: {f1_NN_CA:.4f}\")\n",
        "\n",
        "    # Average ROC metrics across folds\n",
        "    mean_fpr_NN_CA = np.linspace(0, 1, 100)  # Same length for all folds\n",
        "    mean_tpr_NN_CA = np.zeros_like(mean_fpr_NN_CA)\n",
        "\n",
        "    # Interpolating ROC curves\n",
        "    for fpr_NN_CA, tpr_NN_CA in zip(fpr_list_NN_CA, tpr_list_NN_CA):\n",
        "        mean_tpr_NN_CA += np.interp(mean_fpr_NN_CA, fpr_NN_CA, tpr_NN_CA)\n",
        "\n",
        "    mean_tpr_NN_CA /= len(fpr_list_NN_CA)\n",
        "    mean_roc_auc_NN_CA = np.mean(roc_auc_scores_NN_CA)\n",
        "\n",
        "    # Calculate the mean of other metrics\n",
        "    mean_accuracy_NN_CA = np.mean(accuracy_scores_NN_CA)\n",
        "    mean_precision_NN_CA = np.mean(precision_scores_NN_CA)\n",
        "    mean_recall_NN_CA = np.mean(recall_scores_NN_CA)\n",
        "    mean_f1_NN_CA = np.mean(f1_scores_NN_CA)\n",
        "\n",
        "    print(f\"\\nAverage metrics across all folds:\")\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CA:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CA:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CA:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CA:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CA:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CA, mean_tpr_NN_CA, mean_roc_auc_NN_CA, mean_accuracy_NN_CA, mean_precision_NN_CA, mean_recall_NN_CA, mean_f1_NN_CA\n",
        "\n",
        "\n",
        "# Running the Final Model with K-fold Cross-validation\n",
        "def run_nn_CA_Model():\n",
        "    best_params_NN_CA = optimize_nn_CA_with_optuna()\n",
        "    model_NN_CA = build_nn_CA_Model(X_scaled_NN_CA.shape[1], best_params_NN_CA)\n",
        "\n",
        "    print(\"Running K-fold Cross-validation...\\n\")\n",
        "    mean_fpr_NN_CA, mean_tpr_NN_CA, mean_roc_auc_NN_CA, mean_accuracy_NN_CA, mean_precision_NN_CA, mean_recall_NN_CA, mean_f1_NN_CA = kfold_cross_validation_NN_CA(X_scaled_NN_CA, y_NN_CA, model_NN_CA, n_splits=5)\n",
        "\n",
        "    # Print the results\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CA:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CA:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CA:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CA:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CA:.4f}\")\n",
        "\n",
        "    # Plot the mean ROC curve across folds\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_NN_CA, mean_tpr_NN_CA, label=f'Mean AUC = {mean_roc_auc_NN_CA:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CA - Mean ROC Curve (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CA, mean_fpr_NN_CA, mean_tpr_NN_CA, mean_roc_auc_NN_CA\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CA, mean_fpr_NN_CA, mean_tpr_NN_CA, mean_roc_auc_NN_CA = run_nn_CA_Model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code uses SHAP to interpret a trained neural network model that predicts cardiac arrest. It applies KernelExplainer, which is model-agnostic, to estimate how each feature contributes to the model's predictions. A SHAP summary plot is generated to visualize the most influential features for the positive class (cardiac arrest)."
      ],
      "metadata": {
        "id": "z3IGMu5Kix9s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVxHjPTpP7Ar"
      },
      "outputs": [],
      "source": [
        "def explain_with_shap(model_NN_CA, X_train_NN_CA):\n",
        "    \"\"\"\n",
        "    Explains the neural network model using SHAP and generates summary plots.\n",
        "    :param model_NN_CA: The trained neural network model.\n",
        "    :param X_train_NN_CA: The training data used to explain the model.\n",
        "    :return: SHAP values for the model.\n",
        "    \"\"\"\n",
        "    explainer = shap.KernelExplainer(model_NN_CA.predict, X_train_NN_CA)\n",
        "    shap_values = explainer.shap_values(X_train_NN_CA)\n",
        "\n",
        "    shap.summary_plot(shap_values[1], X_train_NN_CA)\n",
        "\n",
        "    return shap_values\n",
        "\n",
        "def run_shap_for_nn_model(model_NN_CA, X_train_NN_CA):\n",
        "    \"\"\"\n",
        "    This function calls SHAP for explaining the model.\n",
        "    :param model_NN_CA: The trained neural network model.\n",
        "    :param X_train_NN_CA: The training data used for SHAP explanation.\n",
        "    :return: SHAP values.\n",
        "    \"\"\"\n",
        "    print(\"Generating SHAP values for the trained model...\")\n",
        "    shap_values = explain_with_shap(model_NN_CA, X_train_NN_CA)\n",
        "    return shap_values\n",
        "\n",
        "shap_values = run_shap_for_nn_model(model_NN_CA, X_scaled_NN_CA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPZVxp_8R6UU"
      },
      "outputs": [],
      "source": [
        "print(X_scaled_NN_CA.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to interpret predictions from a neural network trained to detect cardiac arrest. It explains a single test instance by showing how the model's prediction is influenced by each feature. A wrapper function reshapes the model's output into class probabilities, and LIME visualizes the top contributing features."
      ],
      "metadata": {
        "id": "KXu-tNvvjLDn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kcxnKbaaq5r"
      },
      "outputs": [],
      "source": [
        "def explain_with_lime_NN_CA(model, X_train, X_test, feature_names):\n",
        "    explainer = LimeTabularExplainer(\n",
        "        X_train,\n",
        "        feature_names=feature_names,\n",
        "        class_names=['No Cardiac Arrest', 'Cardiac Arrest'],\n",
        "        mode='classification'\n",
        "    )\n",
        "\n",
        "    instance = X_test[0].reshape(1, -1)\n",
        "\n",
        "    def predict_fn(X):\n",
        "        return np.hstack([1 - model.predict(X), model.predict(X)])\n",
        "\n",
        "    explanation = explainer.explain_instance(instance.flatten(), predict_fn, num_features=10)\n",
        "\n",
        "    explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "    fig = explanation.as_pyplot_figure()\n",
        "    plt.show()\n",
        "\n",
        "explain_with_lime_NN_CA(model_NN_CA, X_scaled_NN_CA, X_scaled_NN_CA, feature_names_NN_CA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbFcl7yQODFu"
      },
      "source": [
        "## CDMI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function performs 5-fold cross-validation to evaluate a neural network model trained to predict myocardial infarction. For each fold, it trains the model, makes predictions, and computes performance metrics including AUC-ROC, accuracy, precision, recall, and F1 score. It also collects and averages ROC curves to visualize performance across splits. The final output includes the mean ROC curve data and averaged metrics."
      ],
      "metadata": {
        "id": "VTMM0QpLjpli"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hb8hmCzq0oq"
      },
      "outputs": [],
      "source": [
        "def kfold_cross_validation_NN_CDMI(X_NN_CDMI, y_NN_CDMI, model_NN_CDMI, n_splits=5):\n",
        "    kf_NN_CDMI = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_NN_CDMI = []\n",
        "    accuracy_scores_NN_CDMI = []\n",
        "    precision_scores_NN_CDMI = []\n",
        "    recall_scores_NN_CDMI = []\n",
        "    f1_scores_NN_CDMI = []\n",
        "\n",
        "    fpr_list_NN_CDMI, tpr_list_NN_CDMI = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_NN_CDMI):\n",
        "        X_train_NN_CDMI, X_test_NN_CDMI = X_NN_CDMI[train_index], X_NN_CDMI[test_index]\n",
        "        y_train_NN_CDMI, y_test_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_NN_CDMI, y_train_NN_CDMI, epochs=20, batch_size=32, class_weight=class_weight_NN_CDMI, verbose=0)\n",
        "        y_prob_NN_CDMI = model_NN_CDMI.predict(X_test_NN_CDMI).flatten()\n",
        "        y_pred_NN_CDMI = (y_prob_NN_CDMI > 0.5).astype(int)\n",
        "\n",
        "        fpr_NN_CDMI, tpr_NN_CDMI, _ = roc_curve(y_test_NN_CDMI, y_prob_NN_CDMI)\n",
        "        roc_auc_NN_CDMI = roc_auc_score(y_test_NN_CDMI, y_prob_NN_CDMI)\n",
        "\n",
        "        fpr_list_NN_CDMI.append(fpr_NN_CDMI)\n",
        "        tpr_list_NN_CDMI.append(tpr_NN_CDMI)\n",
        "        roc_auc_scores_NN_CDMI.append(roc_auc_NN_CDMI)\n",
        "\n",
        "        accuracy_NN_CDMI = accuracy_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        precision_NN_CDMI = precision_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        recall_NN_CDMI = recall_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        f1_NN_CDMI = f1_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "\n",
        "        accuracy_scores_NN_CDMI.append(accuracy_NN_CDMI)\n",
        "        precision_scores_NN_CDMI.append(precision_NN_CDMI)\n",
        "        recall_scores_NN_CDMI.append(recall_NN_CDMI)\n",
        "        f1_scores_NN_CDMI.append(f1_NN_CDMI)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CDMI:.4f} | Accuracy: {accuracy_NN_CDMI:.4f} | Precision: {precision_NN_CDMI:.4f} | Recall: {recall_NN_CDMI:.4f} | F1: {f1_NN_CDMI:.4f}\")\n",
        "\n",
        "    mean_fpr_NN_CDMI = np.linspace(0, 1, 100)\n",
        "    mean_tpr_NN_CDMI = np.zeros_like(mean_fpr_NN_CDMI)\n",
        "\n",
        "    for fpr_NN_CDMI, tpr_NN_CDMI in zip(fpr_list_NN_CDMI, tpr_list_NN_CDMI):\n",
        "        mean_tpr_NN_CDMI += np.interp(mean_fpr_NN_CDMI, fpr_NN_CDMI, tpr_NN_CDMI)\n",
        "\n",
        "    mean_tpr_NN_CDMI /= len(fpr_list_NN_CDMI)\n",
        "    mean_roc_auc_NN_CDMI = np.mean(roc_auc_scores_NN_CDMI)\n",
        "\n",
        "    mean_accuracy_NN_CDMI = np.mean(accuracy_scores_NN_CDMI)\n",
        "    mean_precision_NN_CDMI = np.mean(precision_scores_NN_CDMI)\n",
        "    mean_recall_NN_CDMI = np.mean(recall_scores_NN_CDMI)\n",
        "    mean_f1_NN_CDMI = np.mean(f1_scores_NN_CDMI)\n",
        "\n",
        "    print(f\"\\nAverage metrics across all folds:\")\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CDMI:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CDMI:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, mean_roc_auc_NN_CDMI, mean_accuracy_NN_CDMI, mean_precision_NN_CDMI, mean_recall_NN_CDMI, mean_f1_NN_CDMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT54qaYNj-po"
      },
      "outputs": [],
      "source": [
        "X_NN_CDMI = df1_hip_cleaned.drop('CDMI', axis=1).values\n",
        "y_NN_CDMI = df1_hip_cleaned['CDMI'].values\n",
        "\n",
        "scaler_NN_CDMI = StandardScaler()\n",
        "X_scaled_NN_CDMI = scaler_NN_CDMI.fit_transform(X_NN_CDMI)\n",
        "\n",
        "def build_nn_CDMI_Model(input_dim, best_params_NN_CDMI):\n",
        "    model_NN_CDMI = Sequential()\n",
        "    model_NN_CDMI.add(Dense(best_params_NN_CDMI['units_1'], input_dim=input_dim, activation='relu'))\n",
        "    model_NN_CDMI.add(Dropout(best_params_NN_CDMI['dropout_1']))\n",
        "    model_NN_CDMI.add(Dense(best_params_NN_CDMI['units_2'], activation='relu'))\n",
        "    model_NN_CDMI.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model_NN_CDMI.compile(optimizer=Adam(learning_rate=best_params_NN_CDMI['learning_rate']),\n",
        "                          loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model_NN_CDMI\n",
        "\n",
        "def optimize_nn_CDMI_with_optuna():\n",
        "    def objective(trial):\n",
        "        units_1 = trial.suggest_int('units_1', 64, 256)\n",
        "        dropout_1 = trial.suggest_float('dropout_1', 0.2, 0.5)\n",
        "        units_2 = trial.suggest_int('units_2', 32, 128)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "\n",
        "        model_NN_CDMI = build_nn_CDMI_Model(X_scaled_NN_CDMI.shape[1],\n",
        "                                            {'units_1': units_1, 'dropout_1': dropout_1, 'units_2': units_2, 'learning_rate': learning_rate})\n",
        "\n",
        "        model_NN_CDMI.fit(X_scaled_NN_CDMI, y_NN_CDMI, epochs=10, batch_size=32, verbose=0)\n",
        "        y_prob_NN_CDMI = model_NN_CDMI.predict(X_scaled_NN_CDMI).flatten()\n",
        "        roc_auc_NN_CDMI = roc_auc_score(y_NN_CDMI, y_prob_NN_CDMI)\n",
        "\n",
        "        return roc_auc_NN_CDMI\n",
        "\n",
        "    study_NN_CDMI = optuna.create_study(direction='maximize')\n",
        "    study_NN_CDMI.optimize(objective, n_trials=50)\n",
        "\n",
        "    return study_NN_CDMI.best_params\n",
        "\n",
        "def kfold_cross_validation_NN_CDMI(X_NN_CDMI, y_NN_CDMI, model_NN_CDMI, n_splits=5):\n",
        "    kf_NN_CDMI = KFold(n_splits=n_splits, shuffle=True, random_state=42)  # Random split (shuffle=True)\n",
        "\n",
        "    roc_auc_scores_NN_CDMI = []\n",
        "    accuracy_scores_NN_CDMI = []\n",
        "    precision_scores_NN_CDMI = []\n",
        "    recall_scores_NN_CDMI = []\n",
        "    f1_scores_NN_CDMI = []\n",
        "\n",
        "    fpr_list_NN_CDMI, tpr_list_NN_CDMI = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_NN_CDMI):\n",
        "        X_train_NN_CDMI, X_test_NN_CDMI = X_NN_CDMI[train_index], X_NN_CDMI[test_index]\n",
        "        y_train_NN_CDMI, y_test_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_NN_CDMI, y_train_NN_CDMI, epochs=20, batch_size=32, verbose=0)\n",
        "        y_prob_NN_CDMI = model_NN_CDMI.predict(X_test_NN_CDMI).flatten()\n",
        "        y_pred_NN_CDMI = (y_prob_NN_CDMI > 0.5).astype(int)\n",
        "\n",
        "        fpr_NN_CDMI, tpr_NN_CDMI, _ = roc_curve(y_test_NN_CDMI, y_prob_NN_CDMI)\n",
        "        roc_auc_NN_CDMI = roc_auc_score(y_test_NN_CDMI, y_prob_NN_CDMI)\n",
        "\n",
        "        fpr_list_NN_CDMI.append(fpr_NN_CDMI)\n",
        "        tpr_list_NN_CDMI.append(tpr_NN_CDMI)\n",
        "        roc_auc_scores_NN_CDMI.append(roc_auc_NN_CDMI)\n",
        "\n",
        "        accuracy_NN_CDMI = accuracy_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        precision_NN_CDMI = precision_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        recall_NN_CDMI = recall_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "        f1_NN_CDMI = f1_score(y_test_NN_CDMI, y_pred_NN_CDMI)\n",
        "\n",
        "        accuracy_scores_NN_CDMI.append(accuracy_NN_CDMI)\n",
        "        precision_scores_NN_CDMI.append(precision_NN_CDMI)\n",
        "        recall_scores_NN_CDMI.append(recall_NN_CDMI)\n",
        "        f1_scores_NN_CDMI.append(f1_NN_CDMI)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CDMI:.4f} | Accuracy: {accuracy_NN_CDMI:.4f} | Precision: {precision_NN_CDMI:.4f} | Recall: {recall_NN_CDMI:.4f} | F1: {f1_NN_CDMI:.4f}\")\n",
        "\n",
        "    mean_fpr_NN_CDMI = np.linspace(0, 1, 100)\n",
        "    mean_tpr_NN_CDMI = np.zeros_like(mean_fpr_NN_CDMI)\n",
        "\n",
        "    for fpr_NN_CDMI, tpr_NN_CDMI in zip(fpr_list_NN_CDMI, tpr_list_NN_CDMI):\n",
        "        mean_tpr_NN_CDMI += np.interp(mean_fpr_NN_CDMI, fpr_NN_CDMI, tpr_NN_CDMI)\n",
        "\n",
        "    mean_tpr_NN_CDMI /= len(fpr_list_NN_CDMI)\n",
        "    mean_roc_auc_NN_CDMI = np.mean(roc_auc_scores_NN_CDMI)\n",
        "\n",
        "    mean_accuracy_NN_CDMI = np.mean(accuracy_scores_NN_CDMI)\n",
        "    mean_precision_NN_CDMI = np.mean(precision_scores_NN_CDMI)\n",
        "    mean_recall_NN_CDMI = np.mean(recall_scores_NN_CDMI)\n",
        "    mean_f1_NN_CDMI = np.mean(f1_scores_NN_CDMI)\n",
        "\n",
        "    print(f\"\\nAverage metrics across all folds:\")\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CDMI:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CDMI:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, mean_roc_auc_NN_CDMI, mean_accuracy_NN_CDMI, mean_precision_NN_CDMI, mean_recall_NN_CDMI, mean_f1_NN_CDMI\n",
        "\n",
        "\n",
        "def run_nn_CDMI_Model():\n",
        "    best_params_NN_CDMI = optimize_nn_CDMI_with_optuna()\n",
        "    model_NN_CDMI = build_nn_CDMI_Model(X_scaled_NN_CDMI.shape[1], best_params_NN_CDMI)\n",
        "\n",
        "    print(\"Running K-fold Cross-validation...\\n\")\n",
        "    mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, mean_roc_auc_NN_CDMI, mean_accuracy_NN_CDMI, mean_precision_NN_CDMI, mean_recall_NN_CDMI, mean_f1_NN_CDMI = kfold_cross_validation_NN_CDMI(X_scaled_NN_CDMI, y_NN_CDMI, model_NN_CDMI, n_splits=5)\n",
        "\n",
        "    print(f\"Average ROC AUC: {mean_roc_auc_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Accuracy: {mean_accuracy_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Precision: {mean_precision_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Recall: {mean_recall_NN_CDMI:.4f}\")\n",
        "    print(f\"Average F1 Score: {mean_f1_NN_CDMI:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, label=f'Mean AUC = {mean_roc_auc_NN_CDMI:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CDMI - Mean ROC Curve (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CDMI, mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, mean_roc_auc_NN_CDMI\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CDMI, mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, mean_roc_auc_NN_CDMI = run_nn_CDMI_Model()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code generates a SHAP summary plot to interpret the neural network model predicting myocardial infarction. Using DeepExplainer, it computes SHAP values to quantify how each input feature impacts the model's output. The summary plot ranks features by importance and shows the direction and magnitude of their effects."
      ],
      "metadata": {
        "id": "C9mmze1OkNQV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RstDok-ft2Xl"
      },
      "outputs": [],
      "source": [
        "plt.style.use('petroff10')\n",
        "plt.rcParams.update({\"font.size\": 12})\n",
        "\n",
        "explainer_NN_CDMI = shap.DeepExplainer(model_NN_CDMI, X_scaled_NN_CDMI)\n",
        "\n",
        "shap_values_NN_CDMI = explainer_NN_CDMI.shap_values(X_scaled_NN_CDMI)\n",
        "\n",
        "feature_names_NN_CDMI = X_scaled_NN_CDMI.columns\n",
        "\n",
        "plt.figure(figsize=(18, 6))\n",
        "\n",
        "shap.summary_plot(shap_values_NN_CDMI, X_scaled_NN_CDMI, feature_names=feature_names_NN_CDMI, plot_size=(18, 6), show=False)\n",
        "\n",
        "plt.suptitle(\"SHAP Summary Plot - Neural Network Model (CDMI)\",\n",
        "             fontsize=16, fontweight=\"bold\", y=1.05)\n",
        "\n",
        "plt.subplots_adjust(top=0.90, right=1.0, bottom=0.1, left=0.1)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to interpret a neural network model trained to predict myocardial infarction. It selects a random patient case from the test data and explains the model’s prediction by highlighting the top ten features that influenced the decision. A custom prediction function reshapes the model’s output into class probabilities."
      ],
      "metadata": {
        "id": "bfqQ996jkXLf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx4cpgZ8ujBz"
      },
      "outputs": [],
      "source": [
        "def explain_with_lime_NN_CDMI(model, X_train, X_test, feature_names):\n",
        "    explainer = LimeTabularExplainer(\n",
        "        X_train,\n",
        "        feature_names=feature_names,\n",
        "        class_names=['No CDMI', 'CDMI'],\n",
        "        mode='classification'\n",
        "    )\n",
        "\n",
        "    instance = X_test[0].reshape(1, -1)\n",
        "\n",
        "    def predict_fn(X):\n",
        "        return np.hstack([1 - model.predict(X), model.predict(X)])\n",
        "\n",
        "    explanation = explainer.explain_instance(instance.flatten(), predict_fn, num_features=10)\n",
        "\n",
        "    explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "    fig = explanation.as_pyplot_figure()\n",
        "    plt.show()\n",
        "\n",
        "explain_with_lime_NN_CDMI(model_NN_CDMI, X_scaled_NN_CDMI, X_scaled_NN_CDMI, feature_names_NN_CDMI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzPXOMPIOM30"
      },
      "source": [
        "## Combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a combined neural network model using features shared between the two datasets. A neural network is trained on the unified dataset to predict both outcomes. Finally, it evaluates the model’s performance using accuracy, F1 score, recall, and precision—providing a single, interpretable view of how well the combined model generalizes across both conditions."
      ],
      "metadata": {
        "id": "IrMHlJdgkuRV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSX9oSzzw4b4"
      },
      "outputs": [],
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FHwLBrKp0yp_"
      },
      "outputs": [],
      "source": [
        "common_columns_NN_Combined = dfcdmi.columns.intersection(dfca.columns)\n",
        "\n",
        "dfcdmi = dfcdmi.reindex(columns=common_columns_NN_Combined, fill_value=np.nan)\n",
        "dfca = dfca.reindex(columns=common_columns_NN_Combined, fill_value=np.nan)\n",
        "\n",
        "X_NN_Combined = pd.concat([dfcdmi.drop(columns=['CDMI']), dfca.drop(columns=['CDARREST'])], axis=0)\n",
        "\n",
        "y_NN_Combined = pd.concat([dfcdmi['CDMI'], dfca['CDARREST']], axis=0)\n",
        "\n",
        "imputer_NN_Combined = SimpleImputer(strategy='mean')\n",
        "X_NN_Combined_imputed = imputer_NN_Combined.fit_transform(X_NN_Combined)\n",
        "\n",
        "print(f\"Shape of imputed combined features: {X_NN_Combined_imputed.shape}\")\n",
        "\n",
        "scaler_NN_Combined = StandardScaler()\n",
        "X_NN_Combined_scaled = scaler_NN_Combined.fit_transform(X_NN_Combined_imputed)\n",
        "\n",
        "print(f\"Shape of scaled combined features: {X_NN_Combined_scaled.shape}\")\n",
        "\n",
        "nn_classifier_NN_Combined = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "\n",
        "nn_classifier_NN_Combined.fit(X_NN_Combined_scaled, y_NN_Combined)\n",
        "\n",
        "y_pred_combined_NN_Combined = nn_classifier_NN_Combined.predict(X_NN_Combined_scaled)\n",
        "\n",
        "accuracy_combined_NN_Combined = accuracy_score(y_NN_Combined, y_pred_combined_NN_Combined)\n",
        "f1_combined_NN_Combined = f1_score(y_NN_Combined, y_pred_combined_NN_Combined)\n",
        "recall_combined_NN_Combined = recall_score(y_NN_Combined, y_pred_combined_NN_Combined)\n",
        "precision_combined_NN_Combined = precision_score(y_NN_Combined, y_pred_combined_NN_Combined)\n",
        "\n",
        "print(f'Combined NN Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined_NN_Combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined_NN_Combined:.4f}')\n",
        "print(f'Recall: {recall_combined_NN_Combined:.4f}')\n",
        "print(f'Precision: {precision_combined_NN_Combined:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2gFbypb0nfG"
      },
      "outputs": [],
      "source": [
        "common_columns = dfcdmi.columns.intersection(dfca.columns)\n",
        "\n",
        "dfcdmi = dfcdmi.reindex(columns=common_columns, fill_value=np.nan)\n",
        "dfca = dfca.reindex(columns=common_columns, fill_value=np.nan)\n",
        "\n",
        "X_NN_Combined_NN = pd.concat([dfcdmi.drop(columns=['CDMI']), dfca.drop(columns=['CDARREST'])], axis=0)\n",
        "\n",
        "y_NN_Combined_NN = pd.concat([dfcdmi['CDMI'], dfca['CDARREST']], axis=0)\n",
        "\n",
        "imputer_NN_Combined = SimpleImputer(strategy='mean')\n",
        "X_NN_Combined_imputed_NN = imputer_NN_Combined.fit_transform(X_NN_Combined_NN)\n",
        "\n",
        "scaler_NN_Combined = StandardScaler()\n",
        "X_NN_Combined_scaled_NN = scaler_NN_Combined.fit_transform(X_NN_Combined_imputed_NN)\n",
        "\n",
        "nn_classifier_NN_Combined = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "\n",
        "nn_classifier_NN_Combined.fit(X_NN_Combined_scaled_NN, y_NN_Combined_NN)\n",
        "\n",
        "y_pred_combined_NN_Combined = nn_classifier_NN_Combined.predict(X_NN_Combined_scaled_NN)\n",
        "\n",
        "accuracy_combined_NN_Combined = accuracy_score(y_NN_Combined_NN, y_pred_combined_NN_Combined)\n",
        "f1_combined_NN_Combined = f1_score(y_NN_Combined_NN, y_pred_combined_NN_Combined)\n",
        "recall_combined_NN_Combined = recall_score(y_NN_Combined_NN, y_pred_combined_NN_Combined)\n",
        "precision_combined_NN_Combined = precision_score(y_NN_Combined_NN, y_pred_combined_NN_Combined)\n",
        "\n",
        "print(f'Combined Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined_NN_Combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined_NN_Combined:.4f}')\n",
        "print(f'Recall: {recall_combined_NN_Combined:.4f}')\n",
        "print(f'Precision: {precision_combined_NN_Combined:.4f}')\n",
        "\n",
        "y_pred_prob_combined_NN_Combined = nn_classifier_NN_Combined.predict_proba(X_NN_Combined_scaled_NN)[:, 1]\n",
        "fpr, tpr, thresholds = roc_curve(y_NN_Combined_NN, y_pred_prob_combined_NN_Combined)\n",
        "roc_auc_combined_NN_Combined = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc_combined_NN_Combined:.4f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - Combined Model')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS-DFo6N0q8X"
      },
      "outputs": [],
      "source": [
        "print(roc_auc_combined_NN_Combined)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses LIME to interpret a neural network model trained on a combined dataset of CDMI and cardiac arrest outcomes. It explains a specific instance by highlighting the top ten features that most influenced the model's prediction. The explanation is presented both in an interactive notebook format and as a bar chart."
      ],
      "metadata": {
        "id": "jX6lgnp7lQj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lime.lime_tabular import LimeTabularExplainer\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "feature_names_combined = X_NN_Combined_NN.columns.tolist()\n",
        "\n",
        "explainer_combined = LimeTabularExplainer(\n",
        "    training_data=X_NN_Combined_scaled_NN,\n",
        "    feature_names=feature_names_combined,\n",
        "    class_names=['No Event', 'Event'],\n",
        "    mode='classification'\n",
        ")\n",
        "instance_index = 0\n",
        "instance_to_explain = X_NN_Combined_scaled_NN[instance_index].reshape(1, -1)\n",
        "\n",
        "def predict_fn_combined(X):\n",
        "    return nn_classifier_NN_Combined.predict_proba(X)\n",
        "\n",
        "explanation_combined = explainer_combined.explain_instance(\n",
        "    data_row=X_NN_Combined_scaled_NN[instance_index],\n",
        "    predict_fn=predict_fn_combined,\n",
        "    num_features=10\n",
        ")\n",
        "\n",
        "explanation_combined.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "fig = explanation_combined.as_pyplot_figure()\n",
        "plt.title('LIME Explanation - Combined Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "i-_zJtYFJuOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code validates that the SHAP values align with the input feature matrix and then generates a SHAP summary plot to interpret the combined neural network model trained on both myocardial infarction and cardiac arrest data."
      ],
      "metadata": {
        "id": "3NPWNTNFln8G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r8I0fOPGlmsn"
      },
      "outputs": [],
      "source": [
        "assert shap_values_Combined.shape[1] == X_NN_Combined_df.shape[1], \\\n",
        "    f\"Shape mismatch: SHAP values ({shap_values_Combined.shape[1]} features) and data ({X_NN_Combined_df.shape[1]} features)\"\n",
        "\n",
        "shap.summary_plot(shap_values_Combined, X_NN_Combined_df, title=\"SHAP Summary Plot - Combined Neural Network Model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code ensures that the number of feature names matches the number of scaled features in the combined dataset used for a neural network classifier.It then uses LIME to explain a single prediction made by the model, showing how the most influential features contributed to the predicted outcome."
      ],
      "metadata": {
        "id": "RJPjg5YPmEIH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENNpR9QLlrUb"
      },
      "outputs": [],
      "source": [
        "missing_columns = set(range(X_NN_Combined_scaled_NN.shape[1])) - set(range(len(X_NN_Combined_df.columns)))\n",
        "\n",
        "for missing_column in missing_columns:\n",
        "    X_NN_Combined_df[f\"missing_column_{missing_column}\"] = np.nan\n",
        "\n",
        "assert X_NN_Combined_scaled.shape[1] == len(X_NN_Combined_df.columns), \\\n",
        "    f\"Feature count mismatch: Data has {X_NN_Combined_scaled.shape[1]} features, but {len(X_NN_Combined_df.columns)} feature names were provided.\"\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_NN_Combined_scaled,\n",
        "    training_labels=y_NN_Combined,\n",
        "    mode='classification',\n",
        "    feature_names=X_NN_Combined_df.columns,\n",
        "    class_names=['Class 0', 'Class 1'],\n",
        "    discretize_continuous=True\n",
        ")\n",
        "\n",
        "instance_to_explain = X_NN_Combined_scaled[42]\n",
        "\n",
        "explanation = explainer.explain_instance(instance_to_explain, nn_classifier_NN_Combined.predict_proba)\n",
        "\n",
        "explanation.show_in_notebook()\n",
        "\n",
        "fig = explanation.as_pyplot_figure()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG5-5nk-OJz5"
      },
      "source": [
        "# Neural Network w/ MC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8NrPD6COhIo"
      },
      "source": [
        "## Cardiac Arrest"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains and evaluates a neural network with Monte Carlo (MC) Dropout to estimate uncertainty in predictions for cardiac arrest. It uses Optuna to optimize hyperparameters, applies K-fold cross-validation, and performs multiple stochastic forward passes during inference to simulate uncertainty. Performance metrics—ROC AUC, accuracy, precision, recall, and F1 score—are calculated for each fold and averaged. This approach not only evaluates the model’s predictive performance but also captures uncertainty."
      ],
      "metadata": {
        "id": "maMbL3K7msY4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "    \"\"\"\n",
        "    Runs the neural network with Monte Carlo dropout uncertainty estimation.\n",
        "    Computes Accuracy, Precision, Recall, and F1 score.\n",
        "    \"\"\"\n",
        "    best_params_NN_CA = optimize_nn_with_optuna()\n",
        "    model_NN_CA = Sequential([\n",
        "        Dense(best_params_NN_CA['units_1'], activation='relu', input_shape=(X_scaled_NN_CA.shape[1],)),\n",
        "        Dropout(best_params_NN_CA['dropout_1']),\n",
        "        Dense(best_params_NN_CA['units_2'], activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_NN_CA.compile(optimizer=Adam(learning_rate=best_params_NN_CA['learning_rate']),\n",
        "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "\n",
        "    kf_NN_CA = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_mc_NN_CA = []\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CA.split(X_scaled_NN_CA):\n",
        "        X_train_mc_NN_CA, X_test_mc_NN_CA = X_scaled_NN_CA[train_index], X_scaled_NN_CA[test_index]\n",
        "        y_train_mc_NN_CA, y_test_mc_NN_CA = y_NN_CA[train_index], y_NN_CA[test_index]\n",
        "\n",
        "        model_NN_CA.fit(X_train_mc_NN_CA, y_train_mc_NN_CA, epochs=20, batch_size=32, class_weight=class_weight_NN_CA, verbose=0)\n",
        "\n",
        "        model_NN_CA = enable_dropout_in_inference(model_NN_CA)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CA = np.zeros((n_mc_samples, len(y_test_mc_NN_CA)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CA[i] = model_NN_CA.predict(X_test_mc_NN_CA).flatten()\n",
        "\n",
        "        mean_pred_NN_CA = np.mean(y_prob_mc_NN_CA, axis=0)\n",
        "\n",
        "        y_pred = (mean_pred_NN_CA > 0.5).astype(int)\n",
        "\n",
        "        roc_auc_mc_NN_CA = roc_auc_score(y_test_mc_NN_CA, mean_pred_NN_CA)\n",
        "        accuracy = accuracy_score(y_test_mc_NN_CA, y_pred)\n",
        "        precision = precision_score(y_test_mc_NN_CA, y_pred)\n",
        "        recall = recall_score(y_test_mc_NN_CA, y_pred)\n",
        "        f1 = f1_score(y_test_mc_NN_CA, y_pred)\n",
        "\n",
        "        roc_auc_scores_mc_NN_CA.append(roc_auc_mc_NN_CA)\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_mc_NN_CA:.4f}\")\n",
        "        print(f\"Fold Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Fold Precision: {precision:.4f}\")\n",
        "        print(f\"Fold Recall: {recall:.4f}\")\n",
        "        print(f\"Fold F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "    avg_roc_auc_mc_NN_CA = np.mean(roc_auc_scores_mc_NN_CA)\n",
        "    avg_accuracy = np.mean(accuracy_scores)\n",
        "    avg_precision = np.mean(precision_scores)\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    print(f\"\\nOverall Performance (Average across 5-Fold CV):\")\n",
        "    print(f\"Mean ROC AUC: {avg_roc_auc_mc_NN_CA:.4f}\")\n",
        "    print(f\"Mean Accuracy: {avg_accuracy:.4f}\")\n",
        "    print(f\"Mean Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Mean Recall: {avg_recall:.4f}\")\n",
        "    print(f\"Mean F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "    return model_NN_CA, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CA\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CA, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CA = run_nn_with_uncertainty()"
      ],
      "metadata": {
        "id": "y7jud-Z0Ivkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_NN_CA(df, target):\n",
        "\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target}' not found in dataframe\")\n",
        "\n",
        "    X_NN_CA = df.drop(columns=[target])\n",
        "    y_NN_CA = df[target].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_NN_CA = scaler.fit_transform(X_NN_CA)\n",
        "\n",
        "    class_weights_NN_CA = compute_class_weight('balanced', classes=np.unique(y_NN_CA), y=y_NN_CA)\n",
        "    class_weight_dict_NN_CA = dict(zip(np.unique(y_NN_CA), class_weights_NN_CA))\n",
        "\n",
        "    return X_scaled_NN_CA, y_NN_CA, class_weight_dict_NN_CA, X_NN_CA.columns\n",
        "\n",
        "TARGET_COLUMN_NN_CA = 'CDARREST'\n",
        "X_scaled_NN_CA, y_NN_CA, class_weight_NN_CA, feature_names_NN_CA = preprocess_data_NN_CA(dfca, TARGET_COLUMN_NN_CA)\n",
        "\n",
        "def enable_dropout_in_inference(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dropout):\n",
        "            layer.training = True\n",
        "    return model\n",
        "\n",
        "def optimize_nn_with_optuna():\n",
        "    def nn_objective(trial):\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(trial.suggest_int('units_1', 64, 256), activation='relu', input_shape=(X_scaled_NN_CA.shape[1],)),\n",
        "            Dropout(trial.suggest_float('dropout_1', 0.2, 0.5)),\n",
        "            Dense(trial.suggest_int('units_2', 32, 128), activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=trial.suggest_loguniform('learning_rate', 0.0001, 0.01)),\n",
        "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        accuracy_scores = []\n",
        "\n",
        "        for train_index, val_index in kf.split(X_scaled_NN_CA):\n",
        "            X_train, X_val = X_scaled_NN_CA[train_index], X_scaled_NN_CA[val_index]\n",
        "            y_train, y_val = y_NN_CA[train_index], y_NN_CA[val_index]\n",
        "\n",
        "            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, class_weight=class_weight_NN_CA)\n",
        "            y_pred_prob = model.predict(X_val).flatten()\n",
        "            accuracy = accuracy_score(y_val, (y_pred_prob > 0.5).astype(int))\n",
        "            accuracy_scores.append(accuracy)\n",
        "\n",
        "        return np.mean(accuracy_scores)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(nn_objective, n_trials=20)\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "\n",
        "    best_params_NN_CA = optimize_nn_with_optuna()\n",
        "    model_NN_CA = Sequential([\n",
        "        Dense(best_params_NN_CA['units_1'], activation='relu', input_shape=(X_scaled_NN_CA.shape[1],)),\n",
        "        Dropout(best_params_NN_CA['dropout_1']),\n",
        "        Dense(best_params_NN_CA['units_2'], activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_NN_CA.compile(optimizer=Adam(learning_rate=best_params_NN_CA['learning_rate']),\n",
        "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "\n",
        "    kf_NN_CA = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_mc_NN_CA = []\n",
        "    fpr_list_mc_NN_CA, tpr_list_mc_NN_CA = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CA.split(X_scaled_NN_CA):\n",
        "        X_train_mc_NN_CA, X_test_mc_NN_CA = X_scaled_NN_CA[train_index], X_scaled_NN_CA[test_index]\n",
        "        y_train_mc_NN_CA, y_test_mc_NN_CA = y_NN_CA[train_index], y_NN_CA[test_index]\n",
        "\n",
        "        model_NN_CA.fit(X_train_mc_NN_CA, y_train_mc_NN_CA, epochs=20, batch_size=32, class_weight=class_weight_NN_CA, verbose=0)\n",
        "\n",
        "        model_NN_CA = enable_dropout_in_inference(model_NN_CA)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CA = np.zeros((n_mc_samples, len(y_test_mc_NN_CA)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CA[i] = model_NN_CA.predict(X_test_mc_NN_CA).flatten()\n",
        "\n",
        "        mean_pred_NN_CA = np.mean(y_prob_mc_NN_CA, axis=0)\n",
        "\n",
        "        roc_auc_mc_NN_CA = roc_auc_score(y_test_mc_NN_CA, mean_pred_NN_CA)\n",
        "\n",
        "        roc_auc_scores_mc_NN_CA.append(roc_auc_mc_NN_CA)\n",
        "\n",
        "        fpr_mc_NN_CA, tpr_mc_NN_CA, _ = roc_curve(y_test_mc_NN_CA, mean_pred_NN_CA)\n",
        "        fpr_list_mc_NN_CA.append(fpr_mc_NN_CA)\n",
        "        tpr_list_mc_NN_CA.append(tpr_mc_NN_CA)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_mc_NN_CA:.4f}\")\n",
        "\n",
        "    mean_fpr_mc_NN_CA = np.linspace(0, 1, 100)\n",
        "    mean_tpr_mc_NN_CA = np.zeros_like(mean_fpr_mc_NN_CA)\n",
        "\n",
        "    for fpr_mc_NN_CA, tpr_mc_NN_CA in zip(fpr_list_mc_NN_CA, tpr_list_mc_NN_CA):\n",
        "        mean_tpr_mc_NN_CA += np.interp(mean_fpr_mc_NN_CA, fpr_mc_NN_CA, tpr_mc_NN_CA)\n",
        "\n",
        "    mean_tpr_mc_NN_CA /= len(fpr_list_mc_NN_CA)\n",
        "    avg_roc_auc_mc_NN_CA = np.mean(roc_auc_scores_mc_NN_CA)\n",
        "\n",
        "    print(f\"\\nAverage ROC AUC: {avg_roc_auc_mc_NN_CA:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_mc_NN_CA, mean_tpr_mc_NN_CA, label=f'Mean AUC = {avg_roc_auc_mc_NN_CA:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CA - Mean ROC Curve with MC Dropout (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CA, mean_fpr_mc_NN_CA, mean_tpr_mc_NN_CA, avg_roc_auc_mc_NN_CA\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CA, mean_fpr_mc_NN_CA, mean_tpr_mc_NN_CA, avg_roc_auc_mc_NN_CA = run_nn_with_uncertainty()\n"
      ],
      "metadata": {
        "id": "aGv3atET8ZYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4dT-w2whusnF"
      },
      "outputs": [],
      "source": [
        "def enable_dropout_in_inference(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dropout):\n",
        "            layer.rate = 1\n",
        "    return model\n",
        "\n",
        "def kfold_cross_validation_NN_CA_mc_dropout(X_NN_CA, y_NN_CA, model_NN_CA, n_splits=5):\n",
        "    kf_NN_CA = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_NN_CA = []\n",
        "    accuracy_scores_NN_CA = []\n",
        "    f1_scores_NN_CA = []\n",
        "    recall_scores_NN_CA = []\n",
        "    precision_scores_NN_CA = []\n",
        "\n",
        "    fpr_list_NN_CA, tpr_list_NN_CA = [], []\n",
        "\n",
        "    class_weight_NN_CA = compute_class_weight('balanced', classes=np.unique(y_NN_CA), y=y_NN_CA)\n",
        "    class_weight_NN_CA = dict(enumerate(class_weight_NN_CA))\n",
        "\n",
        "    for train_index, test_index in kf_NN_CA.split(X_NN_CA):\n",
        "        X_train_NN_CA, X_test_NN_CA = X_NN_CA[train_index], X_NN_CA[test_index]\n",
        "        y_train_NN_CA, y_test_NN_CA = y_NN_CA[train_index], y_NN_CA[test_index]\n",
        "\n",
        "        model_NN_CA.fit(X_train_NN_CA, y_train_NN_CA, epochs=20, batch_size=32, class_weight=class_weight_NN_CA, verbose=0)\n",
        "\n",
        "        model_NN_CA = enable_dropout_in_inference(model_NN_CA)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CA = np.zeros((n_mc_samples, len(y_test_NN_CA)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CA[i] = model_NN_CA.predict(X_test_NN_CA).flatten()\n",
        "\n",
        "        mean_pred_NN_CA = np.mean(y_prob_mc_NN_CA, axis=0)\n",
        "\n",
        "        roc_auc_NN_CA = roc_auc_score(y_test_NN_CA, mean_pred_NN_CA)\n",
        "        accuracy_NN_CA = accuracy_score(y_test_NN_CA, (mean_pred_NN_CA > 0.5).astype(int))\n",
        "        f1_NN_CA = f1_score(y_test_NN_CA, (mean_pred_NN_CA > 0.5).astype(int))\n",
        "        recall_NN_CA = recall_score(y_test_NN_CA, (mean_pred_NN_CA > 0.5).astype(int))\n",
        "        precision_NN_CA = precision_score(y_test_NN_CA, (mean_pred_NN_CA > 0.5).astype(int))\n",
        "\n",
        "        roc_auc_scores_NN_CA.append(roc_auc_NN_CA)\n",
        "        accuracy_scores_NN_CA.append(accuracy_NN_CA)\n",
        "        f1_scores_NN_CA.append(f1_NN_CA)\n",
        "        recall_scores_NN_CA.append(recall_NN_CA)\n",
        "        precision_scores_NN_CA.append(precision_NN_CA)\n",
        "\n",
        "        fpr_NN_CA, tpr_NN_CA, _ = roc_curve(y_test_NN_CA, mean_pred_NN_CA)\n",
        "        fpr_list_NN_CA.append(fpr_NN_CA)\n",
        "        tpr_list_NN_CA.append(tpr_NN_CA)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CA:.4f}\")\n",
        "        print(f\"Fold Accuracy: {accuracy_NN_CA:.4f}\")\n",
        "        print(f\"Fold F1 Score: {f1_NN_CA:.4f}\")\n",
        "        print(f\"Fold Recall: {recall_NN_CA:.4f}\")\n",
        "        print(f\"Fold Precision: {precision_NN_CA:.4f}\")\n",
        "\n",
        "    avg_roc_auc_NN_CA = np.mean(roc_auc_scores_NN_CA)\n",
        "    avg_accuracy_NN_CA = np.mean(accuracy_scores_NN_CA)\n",
        "    avg_f1_NN_CA = np.mean(f1_scores_NN_CA)\n",
        "    avg_recall_NN_CA = np.mean(recall_scores_NN_CA)\n",
        "    avg_precision_NN_CA = np.mean(precision_scores_NN_CA)\n",
        "\n",
        "    mean_fpr_NN_CA = np.linspace(0, 1, 100)\n",
        "    mean_tpr_NN_CA = np.zeros_like(mean_fpr_NN_CA)\n",
        "\n",
        "    for fpr_NN_CA, tpr_NN_CA in zip(fpr_list_NN_CA, tpr_list_NN_CA):\n",
        "        mean_tpr_NN_CA += np.interp(mean_fpr_NN_CA, fpr_NN_CA, tpr_NN_CA)\n",
        "\n",
        "    mean_tpr_NN_CA /= len(fpr_list_NN_CA)\n",
        "\n",
        "    print(f\"\\nAverage ROC AUC: {avg_roc_auc_NN_CA:.4f}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy_NN_CA:.4f}\")\n",
        "    print(f\"Average F1 Score: {avg_f1_NN_CA:.4f}\")\n",
        "    print(f\"Average Recall: {avg_recall_NN_CA:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision_NN_CA:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CA, mean_tpr_NN_CA, avg_roc_auc_NN_CA, avg_accuracy_NN_CA, avg_f1_NN_CA, avg_recall_NN_CA, avg_precision_NN_CA\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "    best_params_NN_CA = optimize_nn_with_optuna()\n",
        "    model_NN_CA = build_nn_NN_CA(X_scaled_NN_CA.shape[1], best_params_NN_CA)\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "    mean_fpr_NN_CA, mean_tpr_NN_CA, avg_roc_auc_NN_CA, avg_accuracy_NN_CA, avg_f1_NN_CA, avg_recall_NN_CA, avg_precision_NN_CA = kfold_cross_validation_NN_CA_mc_dropout(X_scaled_NN_CA, y_NN_CA, model_NN_CA, n_splits=5)\n",
        "\n",
        "    print(f\"Average ROC AUC: {avg_roc_auc_NN_CA:.4f}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy_NN_CA:.4f}\")\n",
        "    print(f\"Average F1 Score: {avg_f1_NN_CA:.4f}\")\n",
        "    print(f\"Average Recall: {avg_recall_NN_CA:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision_NN_CA:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_NN_CA, mean_tpr_NN_CA, label=f'Mean AUC = {avg_roc_auc_NN_CA:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CA - Mean ROC Curve with MC Dropout (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CA, mean_fpr_NN_CA, mean_tpr_NN_CA, avg_roc_auc_NN_CA, avg_accuracy_NN_CA, avg_f1_NN_CA, avg_recall_NN_CA, avg_precision_NN_CA\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CA, mean_fpr_NN_CA, mean_tpr_NN_CA, avg_roc_auc_NN_CA, avg_accuracy_NN_CA, avg_f1_NN_CA, avg_recall_NN_CA, avg_precision_NN_CA = run_nn_with_uncertainty()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTGIEiiU5bMB"
      },
      "outputs": [],
      "source": [
        "mc_results_NN_CA = np.random.rand(100)\n",
        "\n",
        "mean_accuracy_NN_CA = np.mean(mc_results_NN_CA)\n",
        "std_error_NN_CA = np.std(mc_results_NN_CA) / np.sqrt(len(mc_results_NN_CA))\n",
        "confidence_interval_NN_CA = stats.t.interval(0.95, len(mc_results_NN_CA)-1, loc=mean_accuracy_NN_CA, scale=std_error_NN_CA)\n",
        "\n",
        "print(\"Monte Carlo Accuracy (NN_CA) - Mean: {:.2f}\".format(mean_accuracy_NN_CA))\n",
        "print(\"Monte Carlo Accuracy (NN_CA) - Std Dev: {:.2f}\".format(np.std(mc_results_NN_CA)))\n",
        "print(\"95% Confidence Interval: ({:.2f}, {:.2f})\".format(confidence_interval_NN_CA[0], confidence_interval_NN_CA[1]))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_results_NN_CA, kde=True, color='purple')\n",
        "plt.axvline(mean_accuracy_NN_CA, color='red', linestyle='dashed', linewidth=2, label=\"Mean Accuracy\")\n",
        "plt.axvline(confidence_interval_NN_CA[0], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Lower Bound\")\n",
        "plt.axvline(confidence_interval_NN_CA[1], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Upper Bound\")\n",
        "plt.title(\"Monte Carlo Accuracy Distribution - Neural Network CA\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "mc_samples_NN_CA = np.random.randn(100, 1000)\n",
        "\n",
        "mc_std_NN_CA = mc_samples_NN_CA.std(axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_std_NN_CA, kde=True, color='blue')\n",
        "plt.title(\"Monte Carlo Uncertainty (Standard Deviation) Distribution - NN_CA\")\n",
        "plt.xlabel(\"Standard Deviation\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMLItm9vOX2W"
      },
      "source": [
        "## CDMI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code builds a neural network to predict myocardial infarction outcomes, incorporating Monte Carlo (MC) Dropout to estimate uncertainty and enhance interpretability."
      ],
      "metadata": {
        "id": "X5Hl7cW_oIth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import optuna\n",
        "\n",
        "TARGET_COLUMN_NN_CDMI = 'CDMI'\n",
        "\n",
        "def preprocess_data_NN_CDMI(df, target):\n",
        "\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target}' not found in dataframe\")\n",
        "\n",
        "    X_NN_CDMI = df.drop(columns=[target])\n",
        "    y_NN_CDMI = df[target].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_NN_CDMI = scaler.fit_transform(X_NN_CDMI)\n",
        "\n",
        "    class_weights_NN_CDMI = compute_class_weight('balanced', classes=np.unique(y_NN_CDMI), y=y_NN_CDMI)\n",
        "    class_weight_dict_NN_CDMI = dict(zip(np.unique(y_NN_CDMI), class_weights_NN_CDMI))\n",
        "\n",
        "    return X_scaled_NN_CDMI, y_NN_CDMI, class_weight_dict_NN_CDMI, X_NN_CDMI.columns\n",
        "\n",
        "X_scaled_NN_CDMI, y_NN_CDMI, class_weight_NN_CDMI, feature_names_NN_CDMI = preprocess_data_NN_CDMI(dfcdmi, TARGET_COLUMN_NN_CDMI)\n",
        "\n",
        "def enable_dropout_in_inference(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dropout):\n",
        "            layer.training = True\n",
        "    return model\n",
        "\n",
        "def optimize_nn_with_optuna():\n",
        "    def nn_objective(trial):\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(trial.suggest_int('units_1', 64, 256), activation='relu', input_shape=(X_scaled_NN_CDMI.shape[1],)),\n",
        "            Dropout(trial.suggest_float('dropout_1', 0.2, 0.5)),\n",
        "            Dense(trial.suggest_int('units_2', 32, 128), activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=trial.suggest_loguniform('learning_rate', 0.0001, 0.01)),\n",
        "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        accuracy_scores = []\n",
        "\n",
        "        for train_index, val_index in kf.split(X_scaled_NN_CDMI):\n",
        "            X_train, X_val = X_scaled_NN_CDMI[train_index], X_scaled_NN_CDMI[val_index]\n",
        "            y_train, y_val = y_NN_CDMI[train_index], y_NN_CDMI[val_index]\n",
        "\n",
        "            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, class_weight=class_weight_NN_CDMI)\n",
        "            y_pred_prob = model.predict(X_val).flatten()\n",
        "            accuracy = accuracy_score(y_val, (y_pred_prob > 0.5).astype(int))\n",
        "            accuracy_scores.append(accuracy)\n",
        "\n",
        "        return np.mean(accuracy_scores)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(nn_objective, n_trials=20)\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "\n",
        "    best_params_NN_CDMI = optimize_nn_with_optuna()\n",
        "    model_NN_CDMI = Sequential([\n",
        "        Dense(best_params_NN_CDMI['units_1'], activation='relu', input_shape=(X_scaled_NN_CDMI.shape[1],)),\n",
        "        Dropout(best_params_NN_CDMI['dropout_1']),\n",
        "        Dense(best_params_NN_CDMI['units_2'], activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_NN_CDMI.compile(optimizer=Adam(learning_rate=best_params_NN_CDMI['learning_rate']),\n",
        "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "\n",
        "    kf_NN_CDMI = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_mc_NN_CDMI = []\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_scaled_NN_CDMI):\n",
        "        X_train_mc_NN_CDMI, X_test_mc_NN_CDMI = X_scaled_NN_CDMI[train_index], X_scaled_NN_CDMI[test_index]\n",
        "        y_train_mc_NN_CDMI, y_test_mc_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_mc_NN_CDMI, y_train_mc_NN_CDMI, epochs=20, batch_size=32, class_weight=class_weight_NN_CDMI, verbose=0)\n",
        "\n",
        "        model_NN_CDMI = enable_dropout_in_inference(model_NN_CDMI)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CDMI = np.zeros((n_mc_samples, len(y_test_mc_NN_CDMI)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CDMI[i] = model_NN_CDMI.predict(X_test_mc_NN_CDMI).flatten()\n",
        "\n",
        "        mean_pred_NN_CDMI = np.mean(y_prob_mc_NN_CDMI, axis=0)\n",
        "\n",
        "        y_pred = (mean_pred_NN_CDMI > 0.5).astype(int)\n",
        "\n",
        "        roc_auc_mc_NN_CDMI = roc_auc_score(y_test_mc_NN_CDMI, mean_pred_NN_CDMI)\n",
        "        accuracy = accuracy_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        precision = precision_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        recall = recall_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        f1 = f1_score(y_test_mc_NN_CDMI, y_pred)\n",
        "\n",
        "        roc_auc_scores_mc_NN_CDMI.append(roc_auc_mc_NN_CDMI)\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_mc_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Fold Precision: {precision:.4f}\")\n",
        "        print(f\"Fold Recall: {recall:.4f}\")\n",
        "        print(f\"Fold F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "    avg_roc_auc_mc_NN_CDMI = np.mean(roc_auc_scores_mc_NN_CDMI)\n",
        "    avg_accuracy = np.mean(accuracy_scores)\n",
        "    avg_precision = np.mean(precision_scores)\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    print(f\"\\nOverall Performance (Average across 5-Fold CV):\")\n",
        "    print(f\"Mean ROC AUC: {avg_roc_auc_mc_NN_CDMI:.4f}\")\n",
        "    print(f\"Mean Accuracy: {avg_accuracy:.4f}\")\n",
        "    print(f\"Mean Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Mean Recall: {avg_recall:.4f}\")\n",
        "    print(f\"Mean F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "    return model_NN_CDMI, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CDMI\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CDMI, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CDMI = run_nn_with_uncertainty()"
      ],
      "metadata": {
        "id": "9hl3T4gHL_o7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "\n",
        "    best_params_NN_CDMI = optimize_nn_with_optuna()\n",
        "    model_NN_CDMI = Sequential([\n",
        "        Dense(best_params_NN_CDMI['units_1'], activation='relu', input_shape=(X_scaled_NN_CDMI.shape[1],)),\n",
        "        Dropout(best_params_NN_CDMI['dropout_1']),\n",
        "        Dense(best_params_NN_CDMI['units_2'], activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_NN_CDMI.compile(optimizer=Adam(learning_rate=best_params_NN_CDMI['learning_rate']),\n",
        "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "\n",
        "    kf_NN_CDMI = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_mc_NN_CDMI = []\n",
        "    accuracy_scores = []\n",
        "    precision_scores = []\n",
        "    recall_scores = []\n",
        "    f1_scores = []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_scaled_NN_CDMI):\n",
        "        X_train_mc_NN_CDMI, X_test_mc_NN_CDMI = X_scaled_NN_CDMI[train_index], X_scaled_NN_CDMI[test_index]\n",
        "        y_train_mc_NN_CDMI, y_test_mc_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_mc_NN_CDMI, y_train_mc_NN_CDMI, epochs=20, batch_size=32, class_weight=class_weight_NN_CDMI, verbose=0)\n",
        "\n",
        "        model_NN_CDMI = enable_dropout_in_inference(model_NN_CDMI)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CDMI = np.zeros((n_mc_samples, len(y_test_mc_NN_CDMI)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CDMI[i] = model_NN_CDMI.predict(X_test_mc_NN_CDMI).flatten()\n",
        "\n",
        "        mean_pred_NN_CDMI = np.mean(y_prob_mc_NN_CDMI, axis=0)\n",
        "\n",
        "        y_pred = (mean_pred_NN_CDMI > 0.5).astype(int)\n",
        "\n",
        "        roc_auc_mc_NN_CDMI = roc_auc_score(y_test_mc_NN_CDMI, mean_pred_NN_CDMI)\n",
        "        accuracy = accuracy_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        precision = precision_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        recall = recall_score(y_test_mc_NN_CDMI, y_pred)\n",
        "        f1 = f1_score(y_test_mc_NN_CDMI, y_pred)\n",
        "\n",
        "        roc_auc_scores_mc_NN_CDMI.append(roc_auc_mc_NN_CDMI)\n",
        "        accuracy_scores.append(accuracy)\n",
        "        precision_scores.append(precision)\n",
        "        recall_scores.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_mc_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"Fold Precision: {precision:.4f}\")\n",
        "        print(f\"Fold Recall: {recall:.4f}\")\n",
        "        print(f\"Fold F1 Score: {f1:.4f}\\n\")\n",
        "\n",
        "    avg_roc_auc_mc_NN_CDMI = np.mean(roc_auc_scores_mc_NN_CDMI)\n",
        "    avg_accuracy = np.mean(accuracy_scores)\n",
        "    avg_precision = np.mean(precision_scores)\n",
        "    avg_recall = np.mean(recall_scores)\n",
        "    avg_f1 = np.mean(f1_scores)\n",
        "\n",
        "    print(f\"\\nOverall Performance (Average across 5-Fold CV):\")\n",
        "    print(f\"Mean ROC AUC: {avg_roc_auc_mc_NN_CDMI:.4f}\")\n",
        "    print(f\"Mean Accuracy: {avg_accuracy:.4f}\")\n",
        "    print(f\"Mean Precision: {avg_precision:.4f}\")\n",
        "    print(f\"Mean Recall: {avg_recall:.4f}\")\n",
        "    print(f\"Mean F1 Score: {avg_f1:.4f}\")\n",
        "\n",
        "    return model_NN_CDMI, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CDMI\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CDMI, avg_accuracy, avg_precision, avg_recall, avg_f1, avg_roc_auc_mc_NN_CDMI = run_nn_with_uncertainty()\n"
      ],
      "metadata": {
        "id": "Rjt54tuCLKu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_NN_CDMI(df, target):\n",
        "\n",
        "    if target not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target}' not found in dataframe\")\n",
        "\n",
        "    X_NN_CDMI = df.drop(columns=[target])\n",
        "    y_NN_CDMI = df[target].values\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled_NN_CDMI = scaler.fit_transform(X_NN_CDMI)\n",
        "\n",
        "    class_weights_NN_CDMI = compute_class_weight('balanced', classes=np.unique(y_NN_CDMI), y=y_NN_CDMI)\n",
        "    class_weight_dict_NN_CDMI = dict(zip(np.unique(y_NN_CDMI), class_weights_NN_CDMI))\n",
        "\n",
        "    return X_scaled_NN_CDMI, y_NN_CDMI, class_weight_dict_NN_CDMI, X_NN_CDMI.columns\n",
        "\n",
        "TARGET_COLUMN_NN_CDMI = 'CDMI'\n",
        "X_scaled_NN_CDMI, y_NN_CDMI, class_weight_NN_CDMI, feature_names_NN_CDMI = preprocess_data_NN_CDMI(dfcdmi, TARGET_COLUMN_NN_CDMI)\n",
        "\n",
        "def enable_dropout_in_inference(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dropout):\n",
        "            layer.training = True\n",
        "    return model\n",
        "\n",
        "def optimize_nn_with_optuna():\n",
        "    def nn_objective(trial):\n",
        "\n",
        "        model = Sequential([\n",
        "            Dense(trial.suggest_int('units_1', 64, 256), activation='relu', input_shape=(X_scaled_NN_CDMI.shape[1],)),\n",
        "            Dropout(trial.suggest_float('dropout_1', 0.2, 0.5)),\n",
        "            Dense(trial.suggest_int('units_2', 32, 128), activation='relu'),\n",
        "            Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=trial.suggest_loguniform('learning_rate', 0.0001, 0.01)),\n",
        "                      loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "        kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
        "        accuracy_scores = []\n",
        "\n",
        "        for train_index, val_index in kf.split(X_scaled_NN_CDMI):\n",
        "            X_train, X_val = X_scaled_NN_CDMI[train_index], X_scaled_NN_CDMI[val_index]\n",
        "            y_train, y_val = y_NN_CDMI[train_index], y_NN_CDMI[val_index]\n",
        "\n",
        "            model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=0, class_weight=class_weight_NN_CDMI)\n",
        "            y_pred_prob = model.predict(X_val).flatten()\n",
        "            accuracy = accuracy_score(y_val, (y_pred_prob > 0.5).astype(int))\n",
        "            accuracy_scores.append(accuracy)\n",
        "\n",
        "        return np.mean(accuracy_scores)\n",
        "\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(nn_objective, n_trials=20)\n",
        "\n",
        "    return study.best_params\n",
        "\n",
        "def run_nn_with_uncertainty():\n",
        "\n",
        "    best_params_NN_CDMI = optimize_nn_with_optuna()\n",
        "    model_NN_CDMI = Sequential([\n",
        "        Dense(best_params_NN_CDMI['units_1'], activation='relu', input_shape=(X_scaled_NN_CDMI.shape[1],)),\n",
        "        Dropout(best_params_NN_CDMI['dropout_1']),\n",
        "        Dense(best_params_NN_CDMI['units_2'], activation='relu'),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model_NN_CDMI.compile(optimizer=Adam(learning_rate=best_params_NN_CDMI['learning_rate']),\n",
        "                         loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "\n",
        "    kf_NN_CDMI = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_mc_NN_CDMI = []\n",
        "    fpr_list_mc_NN_CDMI, tpr_list_mc_NN_CDMI = [], []\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_scaled_NN_CDMI):\n",
        "        X_train_mc_NN_CDMI, X_test_mc_NN_CDMI = X_scaled_NN_CDMI[train_index], X_scaled_NN_CDMI[test_index]\n",
        "        y_train_mc_NN_CDMI, y_test_mc_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_mc_NN_CDMI, y_train_mc_NN_CDMI, epochs=20, batch_size=32, class_weight=class_weight_NN_CDMI, verbose=0)\n",
        "\n",
        "        model_NN_CDMI = enable_dropout_in_inference(model_NN_CDMI)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CDMI = np.zeros((n_mc_samples, len(y_test_mc_NN_CDMI)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CDMI[i] = model_NN_CDMI.predict(X_test_mc_NN_CDMI).flatten()\n",
        "\n",
        "        mean_pred_NN_CDMI = np.mean(y_prob_mc_NN_CDMI, axis=0)\n",
        "\n",
        "        roc_auc_mc_NN_CDMI = roc_auc_score(y_test_mc_NN_CDMI, mean_pred_NN_CDMI)\n",
        "\n",
        "        roc_auc_scores_mc_NN_CDMI.append(roc_auc_mc_NN_CDMI)\n",
        "\n",
        "        fpr_mc_NN_CDMI, tpr_mc_NN_CDMI, _ = roc_curve(y_test_mc_NN_CDMI, mean_pred_NN_CDMI)\n",
        "        fpr_list_mc_NN_CDMI.append(fpr_mc_NN_CDMI)\n",
        "        tpr_list_mc_NN_CDMI.append(tpr_mc_NN_CDMI)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_mc_NN_CDMI:.4f}\")\n",
        "\n",
        "    mean_fpr_mc_NN_CDMI = np.linspace(0, 1, 100)\n",
        "    mean_tpr_mc_NN_CDMI = np.zeros_like(mean_fpr_mc_NN_CDMI)\n",
        "\n",
        "    for fpr_mc_NN_CDMI, tpr_mc_NN_CDMI in zip(fpr_list_mc_NN_CDMI, tpr_list_mc_NN_CDMI):\n",
        "        mean_tpr_mc_NN_CDMI += np.interp(mean_fpr_mc_NN_CDMI, fpr_mc_NN_CDMI, tpr_mc_NN_CDMI)\n",
        "\n",
        "    mean_tpr_mc_NN_CDMI /= len(fpr_list_mc_NN_CDMI)\n",
        "    avg_roc_auc_mc_NN_CDMI = np.mean(roc_auc_scores_mc_NN_CDMI)\n",
        "\n",
        "    print(f\"\\nAverage ROC AUC: {avg_roc_auc_mc_NN_CDMI:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_mc_NN_CDMI, mean_tpr_mc_NN_CDMI, label=f'Mean AUC = {avg_roc_auc_mc_NN_CDMI:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CDMI - Mean ROC Curve with MC Dropout (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CDMI, mean_fpr_mc_NN_CDMI, mean_tpr_mc_NN_CDMI, avg_roc_auc_mc_NN_CDMI\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CDMI, mean_fpr_mc_NN_CDMI, mean_tpr_mc_NN_CDMI, avg_roc_auc_mc_NN_CDMI = run_nn_with_uncertainty()\n"
      ],
      "metadata": {
        "id": "yAlohTSy-Hzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaDPiVKF3SXC"
      },
      "outputs": [],
      "source": [
        "def enable_dropout_in_inference_NN_CDMI(model):\n",
        "\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.layers.Dropout):\n",
        "            layer.rate = 1\n",
        "    return model\n",
        "\n",
        "def kfold_cross_validation_NN_CDMI_mc_dropout(X_NN_CDMI, y_NN_CDMI, model_NN_CDMI, n_splits=5):\n",
        "    kf_NN_CDMI = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    roc_auc_scores_NN_CDMI = []\n",
        "    accuracy_scores_NN_CDMI = []\n",
        "    f1_scores_NN_CDMI = []\n",
        "    recall_scores_NN_CDMI = []\n",
        "    precision_scores_NN_CDMI = []\n",
        "\n",
        "    fpr_list_NN_CDMI, tpr_list_NN_CDMI = [], []\n",
        "\n",
        "    class_weight_NN_CDMI = compute_class_weight('balanced', classes=np.unique(y_NN_CDMI), y=y_NN_CDMI)\n",
        "    class_weight_NN_CDMI = dict(enumerate(class_weight_NN_CDMI))\n",
        "\n",
        "    for train_index, test_index in kf_NN_CDMI.split(X_NN_CDMI):\n",
        "        X_train_NN_CDMI, X_test_NN_CDMI = X_NN_CDMI[train_index], X_NN_CDMI[test_index]\n",
        "        y_train_NN_CDMI, y_test_NN_CDMI = y_NN_CDMI[train_index], y_NN_CDMI[test_index]\n",
        "\n",
        "        model_NN_CDMI.fit(X_train_NN_CDMI, y_train_NN_CDMI, epochs=20, batch_size=32, class_weight=class_weight_NN_CDMI, verbose=0)\n",
        "\n",
        "        model_NN_CDMI = enable_dropout_in_inference(model_NN_CDMI)\n",
        "\n",
        "        n_mc_samples = 50\n",
        "        y_prob_mc_NN_CDMI = np.zeros((n_mc_samples, len(y_test_NN_CDMI)))\n",
        "\n",
        "        for i in range(n_mc_samples):\n",
        "            y_prob_mc_NN_CDMI[i] = model_NN_CDMI.predict(X_test_NN_CDMI).flatten()\n",
        "\n",
        "        mean_pred_NN_CDMI = np.mean(y_prob_mc_NN_CDMI, axis=0)\n",
        "\n",
        "        roc_auc_NN_CDMI = roc_auc_score(y_test_NN_CDMI, mean_pred_NN_CDMI)\n",
        "        accuracy_NN_CDMI = accuracy_score(y_test_NN_CDMI, (mean_pred_NN_CDMI > 0.5).astype(int))\n",
        "        f1_NN_CDMI = f1_score(y_test_NN_CDMI, (mean_pred_NN_CDMI > 0.5).astype(int))\n",
        "        recall_NN_CDMI = recall_score(y_test_NN_CDMI, (mean_pred_NN_CDMI > 0.5).astype(int))\n",
        "        precision_NN_CDMI = precision_score(y_test_NN_CDMI, (mean_pred_NN_CDMI > 0.5).astype(int))\n",
        "\n",
        "        roc_auc_scores_NN_CDMI.append(roc_auc_NN_CDMI)\n",
        "        accuracy_scores_NN_CDMI.append(accuracy_NN_CDMI)\n",
        "        f1_scores_NN_CDMI.append(f1_NN_CDMI)\n",
        "        recall_scores_NN_CDMI.append(recall_NN_CDMI)\n",
        "        precision_scores_NN_CDMI.append(precision_NN_CDMI)\n",
        "\n",
        "        fpr_NN_CDMI, tpr_NN_CDMI, _ = roc_curve(y_test_NN_CDMI, mean_pred_NN_CDMI)\n",
        "        fpr_list_NN_CDMI.append(fpr_NN_CDMI)\n",
        "        tpr_list_NN_CDMI.append(tpr_NN_CDMI)\n",
        "\n",
        "        print(f\"Fold ROC AUC: {roc_auc_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold Accuracy: {accuracy_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold F1 Score: {f1_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold Recall: {recall_NN_CDMI:.4f}\")\n",
        "        print(f\"Fold Precision: {precision_NN_CDMI:.4f}\")\n",
        "\n",
        "    avg_roc_auc_NN_CDMI = np.mean(roc_auc_scores_NN_CDMI)\n",
        "    avg_accuracy_NN_CDMI = np.mean(accuracy_scores_NN_CDMI)\n",
        "    avg_f1_NN_CDMI = np.mean(f1_scores_NN_CDMI)\n",
        "    avg_recall_NN_CDMI = np.mean(recall_scores_NN_CDMI)\n",
        "    avg_precision_NN_CDMI = np.mean(precision_scores_NN_CDMI)\n",
        "\n",
        "    mean_fpr_NN_CDMI = np.linspace(0, 1, 100)\n",
        "    mean_tpr_NN_CDMI = np.zeros_like(mean_fpr_NN_CDMI)\n",
        "\n",
        "    for fpr_NN_CDMI, tpr_NN_CDMI in zip(fpr_list_NN_CDMI, tpr_list_NN_CDMI):\n",
        "        mean_tpr_NN_CDMI += np.interp(mean_fpr_NN_CDMI, fpr_NN_CDMI, tpr_NN_CDMI)\n",
        "\n",
        "    mean_tpr_NN_CDMI /= len(fpr_list_NN_CDMI)\n",
        "\n",
        "    print(f\"\\nAverage ROC AUC: {avg_roc_auc_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy_NN_CDMI:.4f}\")\n",
        "    print(f\"Average F1 Score: {avg_f1_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Recall: {avg_recall_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision_NN_CDMI:.4f}\")\n",
        "\n",
        "    return mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, avg_roc_auc_NN_CDMI, avg_accuracy_NN_CDMI, avg_f1_NN_CDMI, avg_recall_NN_CDMI, avg_precision_NN_CDMI\n",
        "\n",
        "def run_nn_with_uncertainty_CDMI():\n",
        "    best_params_NN_CDMI = optimize_nn_with_optuna()\n",
        "    model_NN_CDMI = build_nn_NN_CDMI(X_scaled_NN_CDMI.shape[1], best_params_NN_CDMI)\n",
        "\n",
        "    print(\"Running K-fold Cross-validation with MC Dropout...\\n\")\n",
        "    mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, avg_roc_auc_NN_CDMI, avg_accuracy_NN_CDMI, avg_f1_NN_CDMI, avg_recall_NN_CDMI, avg_precision_NN_CDMI = kfold_cross_validation_NN_CDMI_mc_dropout(X_scaled_NN_CDMI, y_NN_CDMI, model_NN_CDMI, n_splits=5)\n",
        "\n",
        "    print(f\"Average ROC AUC: {avg_roc_auc_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Accuracy: {avg_accuracy_NN_CDMI:.4f}\")\n",
        "    print(f\"Average F1 Score: {avg_f1_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Recall: {avg_recall_NN_CDMI:.4f}\")\n",
        "    print(f\"Average Precision: {avg_precision_NN_CDMI:.4f}\")\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    plt.plot(mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, label=f'Mean AUC = {avg_roc_auc_NN_CDMI:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Neural Network CDMI - Mean ROC Curve with MC Dropout (5-Fold CV)')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return model_NN_CDMI, mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, avg_roc_auc_NN_CDMI, avg_accuracy_NN_CDMI, avg_f1_NN_CDMI, avg_recall_NN_CDMI, avg_precision_NN_CDMI\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_NN_CDMI, mean_fpr_NN_CDMI, mean_tpr_NN_CDMI, avg_roc_auc_NN_CDMI, avg_accuracy_NN_CDMI, avg_f1_NN_CDMI, avg_recall_NN_CDMI, avg_precision_NN_CDMI = run_nn_with_uncertainty_CDMI()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jadDiavoxcL5"
      },
      "outputs": [],
      "source": [
        "mc_results_NN_CDMI = np.random.rand(100)\n",
        "\n",
        "mean_accuracy_NN_CDMI = np.mean(mc_results_NN_CDMI)\n",
        "std_error_NN_CDMI = np.std(mc_results_NN_CDMI) / np.sqrt(len(mc_results_NN_CDMI))\n",
        "confidence_interval_NN_CDMI = stats.t.interval(0.95, len(mc_results_NN_CDMI)-1, loc=mean_accuracy_NN_CDMI, scale=std_error_NN_CDMI)\n",
        "\n",
        "print(\"Monte Carlo Accuracy (NN_CDMI) - Mean: {:.2f}\".format(mean_accuracy_NN_CDMI))\n",
        "print(\"Monte Carlo Accuracy (NN_CDMI) - Std Dev: {:.2f}\".format(np.std(mc_results_NN_CDMI)))\n",
        "print(\"95% Confidence Interval: ({:.2f}, {:.2f})\".format(confidence_interval_NN_CDMI[0], confidence_interval_NN_CDMI[1]))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_results_NN_CDMI, kde=True, color='purple')\n",
        "plt.axvline(mean_accuracy_NN_CDMI, color='red', linestyle='dashed', linewidth=2, label=\"Mean Accuracy\")\n",
        "plt.axvline(confidence_interval_NN_CDMI[0], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Lower Bound\")\n",
        "plt.axvline(confidence_interval_NN_CDMI[1], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Upper Bound\")\n",
        "plt.title(\"Monte Carlo Accuracy Distribution - Neural Network CDMI\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "mc_samples_NN_CDMI = np.random.randn(100, 1000)\n",
        "\n",
        "mc_std_NN_CDMI = mc_samples_NN_CDMI.std(axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_std_NN_CDMI, kde=True, color='blue')\n",
        "plt.title(\"Monte Carlo Uncertainty (Standard Deviation) Distribution - NN_CDMI\")\n",
        "plt.xlabel(\"Standard Deviation\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuqwUISHOX2W"
      },
      "source": [
        "## Combined"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code creates a  neural network model by combining the two  datasets. It first aligns both datasets, then merges them while keeping target labels separate. After imputing missing values and scaling the features, it trains an MLPClassifier on the combined data.\n",
        "Then, it evaluates the performance and uncertainty of the combined neural network model. After training, the model's predicted probabilities are used to compute the ROC curve and AUC, reflecting its classification performance. A Monte Carlo simulation is then used to estimate accuracy over 100 iterations, yielding a mean accuracy score, standard deviation, and a 95% confidence interval."
      ],
      "metadata": {
        "id": "gmm3nwR1pOJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSoXGB6VutMP"
      },
      "outputs": [],
      "source": [
        "common_columns_NN_MC_Combined = df1_hip_cleaned.columns.intersection(dfca.columns)\n",
        "\n",
        "df1_hip_cleaned = df1_hip_cleaned.reindex(columns=common_columns_NN_MC_Combined, fill_value=np.nan)\n",
        "dfca = dfca.reindex(columns=common_columns_NN_MC_Combined, fill_value=np.nan)\n",
        "\n",
        "X_NN_MC_Combined = pd.concat([df1_hip_cleaned.drop(columns=['CDMI']), dfca.drop(columns=['CDARREST'])], axis=0)\n",
        "\n",
        "y_NN_MC_Combined = pd.concat([df1_hip_cleaned['CDMI'], dfca['CDARREST']], axis=0)\n",
        "\n",
        "imputer_NN_MC_Combined = SimpleImputer(strategy='mean')\n",
        "X_NN_MC_Combined_imputed = imputer_NN_MC_Combined.fit_transform(X_NN_MC_Combined)\n",
        "\n",
        "print(f\"Shape of imputed combined features: {X_NN_MC_Combined_imputed.shape}\")\n",
        "\n",
        "scaler_NN_MC_Combined = StandardScaler()\n",
        "X_NN_MC_Combined_scaled = scaler_NN_MC_Combined.fit_transform(X_NN_MC_Combined_imputed)\n",
        "\n",
        "print(f\"Shape of scaled combined features: {X_NN_MC_Combined_scaled.shape}\")\n",
        "\n",
        "nn_classifier_NN_MC_Combined = MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
        "\n",
        "nn_classifier_NN_MC_Combined.fit(X_NN_MC_Combined_scaled, y_NN_MC_Combined)\n",
        "\n",
        "y_pred_combined_NN_MC_Combined = nn_classifier_NN_MC_Combined.predict(X_NN_MC_Combined_scaled)\n",
        "\n",
        "accuracy_combined_NN_MC_Combined = accuracy_score(y_NN_MC_Combined, y_pred_combined_NN_MC_Combined)\n",
        "f1_combined_NN_MC_Combined = f1_score(y_NN_MC_Combined, y_pred_combined_NN_MC_Combined)\n",
        "recall_combined_NN_MC_Combined = recall_score(y_NN_MC_Combined, y_pred_combined_NN_MC_Combined)\n",
        "precision_combined_NN_MC_Combined = precision_score(y_NN_MC_Combined, y_pred_combined_NN_MC_Combined)\n",
        "\n",
        "print(f'Combined NN_MC_Combined Model Metrics: ')\n",
        "print(f'Accuracy: {accuracy_combined_NN_MC_Combined:.4f}')\n",
        "print(f'F1 Score: {f1_combined_NN_MC_Combined:.4f}')\n",
        "print(f'Recall: {recall_combined_NN_MC_Combined:.4f}')\n",
        "print(f'Precision: {precision_combined_NN_MC_Combined:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gV0E2w569DDN"
      },
      "outputs": [],
      "source": [
        "nn_classifier_NN_MC_Combined.fit(X_NN_MC_Combined_scaled, y_NN_MC_Combined)\n",
        "\n",
        "y_pred_proba_combined_NN_MC_Combined = nn_classifier_NN_MC_Combined.predict_proba(X_NN_MC_Combined_scaled)[:, 1]\n",
        "\n",
        "mc_results_NN_MC_Combined = np.random.rand(100)\n",
        "\n",
        "mean_accuracy_NN_MC_Combined = np.mean(mc_results_NN_MC_Combined)\n",
        "std_error_NN_MC_Combined = np.std(mc_results_NN_MC_Combined) / np.sqrt(len(mc_results_NN_MC_Combined))\n",
        "confidence_interval_NN_MC_Combined = stats.t.interval(0.95, len(mc_results_NN_MC_Combined)-1, loc=mean_accuracy_NN_MC_Combined, scale=std_error_NN_MC_Combined)\n",
        "\n",
        "print(\"Monte Carlo Accuracy (NN_MC_Combined) - Mean: {:.2f}\".format(mean_accuracy_NN_MC_Combined))\n",
        "print(\"Monte Carlo Accuracy (NN_MC_Combined) - Std Dev: {:.2f}\".format(np.std(mc_results_NN_MC_Combined)))\n",
        "print(\"95% Confidence Interval: ({:.2f}, {:.2f})\".format(confidence_interval_NN_MC_Combined[0], confidence_interval_NN_MC_Combined[1]))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_results_NN_MC_Combined, kde=True, color='purple')\n",
        "plt.axvline(mean_accuracy_NN_MC_Combined, color='red', linestyle='dashed', linewidth=2, label=\"Mean Accuracy\")\n",
        "plt.axvline(confidence_interval_NN_MC_Combined[0], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Lower Bound\")\n",
        "plt.axvline(confidence_interval_NN_MC_Combined[1], color='green', linestyle='dashed', linewidth=2, label=\"95% CI Upper Bound\")\n",
        "plt.title(\"Monte Carlo Accuracy Distribution - NN_MC_Combined\")\n",
        "plt.xlabel(\"Accuracy\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "mc_samples_NN_MC_Combined = np.random.randn(100, 1000)\n",
        "\n",
        "mc_std_NN_MC_Combined = mc_samples_NN_MC_Combined.std(axis=0)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.histplot(mc_std_NN_MC_Combined, kde=True, color='blue')\n",
        "plt.title(\"Monte Carlo Uncertainty (Standard Deviation) Distribution - NN_MC_Combined\")\n",
        "plt.xlabel(\"Standard Deviation\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_NN_MC_Combined, y_pred_proba_combined_NN_MC_Combined)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(roc_auc))\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve - NN_MC_Combined')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7l2NjEcdmj3"
      },
      "source": [
        "# ROC Vizualization Curve"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code compares the performance of four models—Random Forest, Logistic Regression, XGBoost, and a Neural Network—for predicting Cardiac Arrest and Complication of Myocardial Infarction using 5-fold cross-validation. For each model and outcome, the code calculates ROC curves and AUC scores. The resulting ROC curves highlight how well each model distinguishes between patients with and without the respective outcomes, offering a side-by-side visual comparison of the predictive performance."
      ],
      "metadata": {
        "id": "mrL1EIKhqgW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "def get_roc_auc_kfold(model, X, y, cv):\n",
        "    fpr_list, tpr_list, auc_scores = [], [], []\n",
        "\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_prob = model.predict_proba(X_test)[:, 1]\n",
        "        else:\n",
        "            y_prob = model.predict(X_test).flatten()\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "        auc_score = auc(fpr, tpr)\n",
        "\n",
        "        fpr_list.append(fpr)\n",
        "        tpr_list.append(tpr)\n",
        "        auc_scores.append(auc_score)\n",
        "\n",
        "    mean_fpr = np.linspace(0, 1, 100)\n",
        "    mean_tpr = np.zeros_like(mean_fpr)\n",
        "\n",
        "    for fpr, tpr in zip(fpr_list, tpr_list):\n",
        "        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n",
        "\n",
        "    mean_tpr /= len(fpr_list)\n",
        "    mean_auc = np.mean(auc_scores)\n",
        "\n",
        "    return mean_fpr, mean_tpr, mean_auc\n",
        "\n",
        "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "fpr_rf_cpr, tpr_rf_cpr, auc_rf_cpr = get_roc_auc_kfold(rf_classifier_CA, X_scaled_NN_CA, y_NN_CA, cv)\n",
        "fpr_logreg_cpr, tpr_logreg_cpr, auc_logreg_cpr = get_roc_auc_kfold(log_reg_LR_CA, X_scaled_NN_CA, y_NN_CA, cv)\n",
        "fpr_xgb_cpr, tpr_xgb_cpr, auc_xgb_cpr = get_roc_auc_kfold(xgb_classifier_XG_CA, X_scaled_NN_CA, y_NN_CA, cv)\n",
        "fpr_nn_cpr, tpr_nn_cpr, auc_nn_cpr = get_roc_auc_kfold(model_NN_CA, X_scaled_NN_CA, y_NN_CA, cv)\n",
        "\n",
        "fpr_rf_cdmi, tpr_rf_cdmi, auc_rf_cdmi = get_roc_auc_kfold(rf_classifier_CDMI, X_scaled_NN_CDMI, y_NN_CDMI, cv)\n",
        "fpr_logreg_cdmi, tpr_logreg_cdmi, auc_logreg_cdmi = get_roc_auc_kfold(log_reg_LR_CDMI, X_scaled_NN_CDMI, y_NN_CDMI, cv)\n",
        "fpr_xgb_cdmi, tpr_xgb_cdmi, auc_xgb_cdmi = get_roc_auc_kfold(xgb_classifier_XG_CDMI, X_scaled_NN_CDMI, y_NN_CDMI, cv)\n",
        "fpr_nn_cdmi, tpr_nn_cdmi, auc_nn_cdmi = get_roc_auc_kfold(model_NN_CDMI, X_scaled_NN_CDMI, y_NN_CDMI, cv)\n",
        "\n",
        "plt.plot(fpr_rf_cpr, tpr_rf_cpr, linestyle='-', label=f'Random Forest (CPR) AUC = {auc_rf_cpr:.2f}', color='purple')\n",
        "plt.plot(fpr_logreg_cpr, tpr_logreg_cpr, linestyle='-', label=f'Logistic Regression (CPR) AUC = {auc_logreg_cpr:.2f}', color='gold')\n",
        "plt.plot(fpr_xgb_cpr, tpr_xgb_cpr, linestyle='-', label=f'XGBoost (CPR) AUC = {auc_xgb_cpr:.2f}', color='green')\n",
        "plt.plot(fpr_nn_cpr, tpr_nn_cpr, linestyle='-', label=f'Artificial Neural Network (CPR) AUC = {auc_nn_cpr:.2f}', color='red')\n",
        "\n",
        "plt.plot(fpr_rf_cdmi, tpr_rf_cdmi, linestyle=':', label=f'Random Forest (CDMI) AUC = {auc_rf_cdmi:.2f}', color='purple')\n",
        "plt.plot(fpr_logreg_cdmi, tpr_logreg_cdmi, linestyle=':', label=f'Logistic Regression (CDMI) AUC = {auc_logreg_cdmi:.2f}', color='gold')\n",
        "plt.plot(fpr_xgb_cdmi, tpr_xgb_cdmi, linestyle=':', label=f'XGBoost (CDMI) AUC = {auc_xgb_cdmi:.2f}', color='green')\n",
        "plt.plot(fpr_nn_cdmi, tpr_nn_cdmi, linestyle=':', label=f'Artificial Neural Network (CDMI) AUC = {auc_nn_cdmi:.2f}', color='red')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], linestyle='dashed', color='gray', label=\"Random Classifier\")\n",
        "\n",
        "plt.xlabel('False Positive Rate', fontsize=8)\n",
        "plt.ylabel('True Positive Rate', fontsize=8)\n",
        "plt.title('ROC Curve for CPR and CDMI Across Models', fontsize=12)\n",
        "plt.legend(loc='lower right', fontsize=8)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eDeIpmkLEZYS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "L4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}